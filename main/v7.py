import os
import tomli
import streamlit as st
import http.client
import json
import logging
import re
import time
import anthropic
import requests
from langchain_community.chat_models import ChatPerplexity
from langchain.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_core.messages import AIMessage
from langchain.globals import set_verbose, get_verbose
from konlpy.tag import Okt
from dataclasses import dataclass
from typing import List, Dict, Optional
from tavily import TavilyClient

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def load_secrets():
    """ì‹œí¬ë¦¿ í‚¤ ë¡œë“œ í•¨ìˆ˜"""
    with open("secrets.toml", "rb") as f:
        return tomli.load(f)

SECRETS = load_secrets()
PERPLEXITY_API_KEY = SECRETS["api"]["perplexity"]
SERPER_API_KEY = SECRETS["api"]["serper"]
TAVILY_API_KEY = SECRETS["api"]["tavily"]
ANTHROPIC_API_KEY = SECRETS["api"]["anthropic"]

@dataclass
class ConversationState:
    """ëŒ€í™” ìƒíƒœë¥¼ ê´€ë¦¬í•˜ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    step: str = "keyword"
    data: Dict = None
    
    def __post_init__(self):
        if self.data is None:
            self.data = {}

class KeywordAnalyzer:
    """Perplexityë¥¼ ì‚¬ìš©í•œ í‚¤ì›Œë“œ ë¶„ì„ í´ë˜ìŠ¤"""
    def __init__(self):
        self.llm = ChatPerplexity(
            model="sonar-pro",
            api_key=PERPLEXITY_API_KEY,
            temperature=0.7
        )
        
    def analyze_keyword(self, keyword: str) -> dict:
        """í‚¤ì›Œë“œ ë¶„ì„ ìˆ˜í–‰"""
        prompt = ChatPromptTemplate.from_template("""
        ë‹¤ìŒ í‚¤ì›Œë“œë¥¼ SEO ê´€ì ì—ì„œ ë¶„ì„í•´ì£¼ì„¸ìš”:
        í‚¤ì›Œë“œ: {keyword}

        ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë¶„ì„ ê²°ê³¼ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:

        1. ì£¼ìš” ê²€ìƒ‰ ì˜ë„: 
        (2-3ë¬¸ì¥ìœ¼ë¡œ ì´ í‚¤ì›Œë“œë¥¼ ê²€ìƒ‰í•˜ëŠ” ì‚¬ëŒë“¤ì˜ ì£¼ìš” ì˜ë„ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”)

        2. ê²€ìƒ‰ìê°€ ì–»ê³ ì í•˜ëŠ” ì •ë³´:
        (ê°€ì¥ ì¤‘ìš”í•œ 3ê°€ì§€ë§Œ bullet pointë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”)
        - 
        - 
        - 

        3. ê²€ìƒ‰ìê°€ ê²ªê³  ìˆëŠ” ë¶ˆí¸í•¨ì´ë‚˜ ì–´ë ¤ì›€:
        (ê°€ì¥ ì¼ë°˜ì ì¸ 3ê°€ì§€ ì–´ë ¤ì›€ë§Œ bullet pointë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”)
        - 
        - 
        - 
        """)
        
        result = self.llm.invoke(prompt.format(keyword=keyword))
        return self._parse_analysis_result(result)
    
    def suggest_subtopics(self, keyword: str) -> List[str]:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ì†Œì œëª© ì¶”ì²œ"""
        prompt = ChatPromptTemplate.from_template("""
        ê²€ìƒ‰ í‚¤ì›Œë“œ '{keyword}'ì— ëŒ€í•œ ë¸”ë¡œê·¸ ì†Œì œëª© 4ê°œë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”.

        ì¡°ê±´:
        1. ëª¨ë“  ì†Œì œëª©ì€ ë°˜ë“œì‹œ '{keyword}'ì™€ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ë˜ì–´ì•¼ í•¨
        2. ì†Œì œëª©ë“¤ì€ ë…¼ë¦¬ì  ìˆœì„œë¡œ êµ¬ì„±
        3. ê° ì†Œì œëª©ì€ ê²€ìƒ‰ìì˜ ì‹¤ì œ ê³ ë¯¼/ê¶ê¸ˆì¦ì„ í•´ê²°í•  ìˆ˜ ìˆëŠ” ë‚´ìš©
        4. ì „ì²´ì ìœ¼ë¡œ '{keyword}'ì— ëŒ€í•œ í¬ê´„ì  ì´í•´ë¥¼ ì œê³µí•  ìˆ˜ ìˆëŠ” êµ¬ì„±

        í˜•ì‹:
        1. [ì²« ë²ˆì§¸ ì†Œì œëª©]: ê¸°ì´ˆ/ê°œìš”
        2. [ë‘ ë²ˆì§¸ ì†Œì œëª©]: ì£¼ìš” ì •ë³´/íŠ¹ì§•
        3. [ì„¸ ë²ˆì§¸ ì†Œì œëª©]: ì‹¤ìš©ì  íŒ/ë°©ë²•
        4. [ë„¤ ë²ˆì§¸ ì†Œì œëª©]: ì„ íƒ/ê´€ë¦¬ ë°©ë²•
        """)
        
        result = self.llm.invoke(prompt.format(keyword=keyword))
        return self._parse_subtopics(result)

    def _parse_analysis_result(self, result) -> dict:
        """ë¶„ì„ ê²°ê³¼ íŒŒì‹±"""
        content = str(result.content) if hasattr(result, 'content') else str(result)
        
        # ì„¹ì…˜ë³„ ë‚´ìš© ì¶”ì¶œ
        sections = content.split('\n\n')
        main_intent = ""
        info_needed = []
        pain_points = []
        
        for section in sections:
            if '1. ì£¼ìš” ê²€ìƒ‰ ì˜ë„:' in section:
                main_intent = section.split('ì£¼ìš” ê²€ìƒ‰ ì˜ë„:')[1].strip()
            elif '2. ê²€ìƒ‰ìê°€ ì–»ê³ ì í•˜ëŠ” ì •ë³´:' in section:
                info_lines = section.split('\n')[1:]
                info_needed = [line.strip('- ').strip() for line in info_lines if line.strip().startswith('-')]
            elif '3. ê²€ìƒ‰ìê°€ ê²ªê³  ìˆëŠ” ë¶ˆí¸í•¨ì´ë‚˜ ì–´ë ¤ì›€:' in section:
                pain_lines = section.split('\n')[1:]
                pain_points = [line.strip('- ').strip() for line in pain_lines if line.strip().startswith('-')]
        
        return {
            'raw_text': content,
            'main_intent': main_intent,
            'info_needed': info_needed,
            'pain_points': pain_points
        }
    
    def _parse_subtopics(self, result) -> List[str]:
        """ì†Œì œëª© íŒŒì‹±"""
        content = str(result.content) if hasattr(result, 'content') else str(result)
        subtopics = []
        
        for line in content.split('\n'):
            if line.strip() and line[0].isdigit() and '. ' in line:
                subtitle = line.split('. ', 1)[1].strip()
                if subtitle:
                    subtopics.append(subtitle)
        
        return subtopics[:4]  # ìµœëŒ€ 4ê°œì˜ ì†Œì œëª©ë§Œ ë°˜í™˜

class ResearchCollector:
    """Serperì™€ Tavilyë¥¼ ì‚¬ìš©í•œ ì—°êµ¬ ìë£Œ ìˆ˜ì§‘ í´ë˜ìŠ¤"""
    def __init__(self):
        self.serper_api_key = SERPER_API_KEY
        self.tavily_api_key = TAVILY_API_KEY
    
    def _get_statistics_data(self, query: str) -> List[Dict]:
        """í†µê³„ ë°ì´í„° ìˆ˜ì§‘"""
        try:
            # í†µê³„ ë°ì´í„° ì¶”ì¶œì„ ìœ„í•œ ê²€ìƒ‰
            response = self.llm.invoke(f"Find specific statistics, numbers and data about: {query}")
            content = str(response.content) if hasattr(response, 'content') else str(response)
            
            # í†µê³„ ë°ì´í„° ì¶”ì¶œ
            statistics = self._extract_statistics_from_text(content)
            
            return statistics
        except Exception as e:
            logger.error(f"í†µê³„ ë°ì´í„° ìˆ˜ì§‘ ì˜¤ë¥˜: {str(e)}")
            return []

    def collect_research(self, keyword: str, subtopics: List[str]) -> Dict:
        """í‚¤ì›Œë“œì™€ ì†Œì œëª© ê´€ë ¨ ì—°êµ¬ ìë£Œ ìˆ˜ì§‘"""
        all_results = {
            'news': [],
            'academic': [],
            'perplexity': [],
            'statistics': []
        }
        
        # 1. í‚¤ì›Œë“œ ê´€ë ¨ ìë£Œ ìˆ˜ì§‘
        search_queries = [
            f"{keyword} í†µê³„",
            f"{keyword} ì—°êµ¬ê²°ê³¼",
            f"{keyword} ìµœì‹  ë™í–¥",
            f"{keyword} ì‹œì¥ í˜„í™©",
            f"{keyword} íŠ¸ë Œë“œ"
        ]
        
        # 2. ì†Œì œëª© ê´€ë ¨ ìë£Œ ìˆ˜ì§‘
        for subtopic in subtopics:
            search_queries.extend([
                f"{keyword} {subtopic}",
                f"{keyword} {subtopic} í†µê³„",
                f"{keyword} {subtopic} ì—°êµ¬"
            ])
        
        for query in search_queries:
            # ë‰´ìŠ¤ ê²€ìƒ‰ (ìµœì‹ ìˆœìœ¼ë¡œ ì •ë ¬)
            news_results = self._get_news_from_serper(query)
            all_results['news'].extend(news_results)
            
            # í•™ìˆ  ìë£Œ ê²€ìƒ‰
            academic_results = self._get_academic_from_tavily(query)
            all_results['academic'].extend(academic_results)
            
            # Perplexity ê²€ìƒ‰
            perplexity_results = self._get_perplexity_search(query)
            all_results['perplexity'].extend(perplexity_results)
        
        # 3. í†µê³„ ë°ì´í„° ì¶”ì¶œ (ëª¨ë“  ìˆ˜ì§‘ëœ ìë£Œì—ì„œ)
        for category in ['news', 'academic', 'perplexity']:
            for item in all_results[category]:
                statistics = self._extract_statistics_from_text(
                    item.get('title', '') + ' ' + item.get('snippet', '')
                )
                if statistics:
                    for stat in statistics:
                        stat['source_url'] = item.get('url', '')
                        stat['source_title'] = item.get('title', '')
                        # ì¶œì²˜ì™€ ë‚ ì§œ ì •ë³´ ì¶”ê°€
                        stat['source'] = item.get('source', '')
                        stat['date'] = item.get('date', '')
                    all_results['statistics'].extend(statistics)
        
        # 4. ì¤‘ë³µ ì œê±° ë° ìµœì‹ ìˆœ ì •ë ¬
        for category in all_results:
            all_results[category] = self._deduplicate_results(all_results[category])
            # ë‚ ì§œ ì •ë³´ê°€ ìˆëŠ” ê²½ìš° ìµœì‹ ìˆœ ì •ë ¬
            if category in ['news', 'statistics']:
                all_results[category].sort(
                    key=lambda x: x.get('date', ''), 
                    reverse=True
                )
        
        return all_results

    def _get_perplexity_search(self, query: str, limit: int = 5) -> List[Dict]:
        """Perplexityë¥¼ í†µí•œ ê²€ìƒ‰"""
        try:
            self.llm = ChatPerplexity(
                model="sonar-pro",
                api_key=PERPLEXITY_API_KEY,
                temperature=0.7
            )
            # í†µê³„ ë°ì´í„°ì™€ ê´€ë ¨ëœ ê²€ìƒ‰ ìˆ˜í–‰
            stats_prompt = f"Find statistics, numbers, research data, news, and articles about: {query}"
            response = self.llm.invoke(stats_prompt)
            content = str(response.content) if hasattr(response, 'content') else str(response)
            
            # URL ì¶”ì¶œ
            urls = re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', content)
            
            results = []
            for url in urls[:limit]:
                results.append({
                    'title': f"Perplexity search result for: {query}",
                    'url': url,
                    'snippet': content[:200],
                    'source': 'Perplexity'
                })
            
            return results
        except Exception as e:
            logger.error(f"Perplexity ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
            return []

    def _extract_statistics_from_text(self, text: str) -> List[Dict]:
        """í…ìŠ¤íŠ¸ì—ì„œ í†µê³„ ë°ì´í„° ì¶”ì¶œ"""
        statistics = []
        
        # ìˆ«ì/í¼ì„¼íŠ¸ íŒ¨í„´
        patterns = [
            r'(\d+(?:,\d{3})*(?:\.\d+)?)\s*(?:ëª…|ê°œ|ì›|ë‹¬ëŸ¬|ìœ„|ë°°|ì²œ|ë§Œ|ì–µ|%|í¼ì„¼íŠ¸)',  # í•œê¸€ ë‹¨ìœ„
            r'(\d+(?:,\d{3})*(?:\.\d+)?)\s*(?:people|users|dollars|percent|%)',  # ì˜ë¬¸ ë‹¨ìœ„
            r'(\d+(?:\.\d+)?)[%ï¼…]'  # í¼ì„¼íŠ¸ ê¸°í˜¸
        ]
        
        for pattern in patterns:
            matches = re.finditer(pattern, text)
            for match in matches:
                # í†µê³„ ë°ì´í„°ì˜ ì „í›„ ë¬¸ë§¥ ì¶”ì¶œ (ìµœëŒ€ 100ì)
                start = max(0, match.start() - 50)
                end = min(len(text), match.end() + 50)
                context = text[start:end].strip()
                
                statistics.append({
                    'value': match.group(0),
                    'context': context,
                    'pattern_type': 'numeric' if '%' not in match.group(0) else 'percentage'
                })
        
        return statistics

    def _get_news_from_serper(self, query: str) -> List[Dict]:
        """Serperë¥¼ í†µí•œ ë‰´ìŠ¤ ê²€ìƒ‰"""
        try:
            conn = http.client.HTTPSConnection("google.serper.dev")
            payload = json.dumps({
                "q": query,
                "gl": "kr",
                "hl": "ko",
                "type": "news",
                "timerange": "y"
            })
            headers = {
                'X-API-KEY': self.serper_api_key,
                'Content-Type': 'application/json'
            }
            
            conn.request("POST", "/news", payload, headers)
            res = conn.getresponse()
            data = json.loads(res.read().decode("utf-8"))
            
            results = []
            for item in data.get("news", [])[:3]:  # ìƒìœ„ 3ê°œ ê²°ê³¼ë§Œ ì‚¬ìš©
                results.append({
                    'title': item.get('title', ''),
                    'url': item.get('link', ''),
                    'snippet': item.get('snippet', ''),
                    'date': item.get('date', ''),
                    'source': item.get('source', '')
                })
            
            return results
            
        except Exception as e:
            logger.error(f"Serper API ì˜¤ë¥˜: {str(e)}")
            return []

    def _get_academic_from_tavily(self, query: str) -> List[Dict]:
        """Tavilyë¥¼ í†µí•œ í•™ìˆ  ìë£Œ ê²€ìƒ‰"""
        try:
            client = TavilyClient(api_key=self.tavily_api_key)
            response = client.search(
                query=f"{query} research paper statistics",
                search_depth="advanced",
                time_range="year",
                include_answer="true"
            )
            
            # Tavily ì‘ë‹µì—ì„œ academic_papers í•„í„°ë§
            results = []
            for result in response.get('results', [])[:3]:  # ìƒìœ„ 3ê°œ ê²°ê³¼ë§Œ ì‚¬ìš©
                if any(domain in result.get('url', '').lower() for domain in 
                      ['scholar.google', 'researchgate', 'academia.edu', 'sci-hub', 
                       'pubmed', 'arxiv', 'springer', 'sciencedirect']):
                    results.append({
                        'title': result.get('title', ''),
                        'url': result.get('url', ''),
                        'snippet': result.get('content', ''),
                        'score': result.get('score', 0)
                    })
            
            return results
            
        except Exception as e:
            logger.error(f"Tavily API ì˜¤ë¥˜: {str(e)}")
            return []

    def _deduplicate_results(self, results: List[Dict]) -> List[Dict]:
        """ê²€ìƒ‰ ê²°ê³¼ ì¤‘ë³µ ì œê±°"""
        seen_urls = set()
        unique_results = []
        
        for result in results:
            url = result.get('url', '')
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique_results.append(result)
        
        return unique_results

class ContentGenerator:
    """Claudeë¥¼ ì‚¬ìš©í•œ ì½˜í…ì¸  ìƒì„± ë° ìµœì í™” í´ë˜ìŠ¤"""
    def __init__(self):
        self.claude = ChatAnthropic(
            anthropic_api_key=ANTHROPIC_API_KEY,
            model="claude-3-opus-20240229",
            temperature=0.7,
            max_tokens=4096
        )
        self.okt = Okt()

    def generate_content(self, data: Dict) -> str:
        """ì½˜í…ì¸  ìƒì„±"""
        MAX_RETRIES = 3
        RETRY_DELAY = 2  # ì´ˆ

        for attempt in range(MAX_RETRIES):
            try:
                prompt = self._create_content_prompt(data)
                response = self.claude.invoke(prompt)
                content = str(response)
                
                # ìµœì í™” í•„ìš” ì—¬ë¶€ í™•ì¸
                if self._needs_optimization(content, data['keyword']):
                    content = self.optimize_content(content, data)
                
                # ì°¸ê³ ìë£Œê°€ ìˆì„ ê²½ìš°ì—ë§Œ ì¶œì²˜ ì¶”ê°€
                if isinstance(data.get('research_data'), dict):
                    content = self.add_references(content, data['research_data'])
                    
                return content

            except anthropic.InternalServerError as e:
                if 'overloaded_error' in str(e):
                    if attempt < MAX_RETRIES - 1:  # ë§ˆì§€ë§‰ ì‹œë„ê°€ ì•„ë‹ˆë©´
                        st.warning(f"ì„œë²„ê°€ í˜¼ì¡í•©ë‹ˆë‹¤. {RETRY_DELAY}ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤... ({attempt + 1}/{MAX_RETRIES})")
                        time.sleep(RETRY_DELAY)
                        continue
                logger.error(f"ì½˜í…ì¸  ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                raise e
            except Exception as e:
                logger.error(f"ì½˜í…ì¸  ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                raise e

    def optimize_content(self, content: str, data: Dict) -> str:
        """ì½˜í…ì¸  ìµœì í™”"""
        try:
            optimization_prompt = self._create_optimization_prompt(content, data)
            response = self.claude.invoke(optimization_prompt)
            
            # Claudeì˜ ì‘ë‹µ ì²˜ë¦¬
            if hasattr(response, 'content'):
                return response.content
            else:
                return str(response)

        except Exception as e:
            logger.error(f"ì½˜í…ì¸  ìµœì í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            raise e

    def _find_citation_in_content(self, content: str, source_info: Dict) -> bool:
        """ë³¸ë¬¸ì—ì„œ ì¸ìš© ì—¬ë¶€ í™•ì¸"""
        content_lower = content.lower()
        title = source_info.get('title', '').lower()
        snippet = source_info.get('snippet', '').lower()
        
        # ì¸ìš© íŒ¨í„´ í™•ì¸
        citation_patterns = [
            "ì—°êµ¬ì— ë”°ë¥´ë©´",
            "í†µê³„ì— ì˜í•˜ë©´",
            "ì¡°ì‚¬ ê²°ê³¼",
            "ë³´ê³ ì„œì— ë”°ë¥´ë©´",
            "ë°œí‘œí•œ ìë£Œì— ë”°ë¥´ë©´",
            "ì˜ ì—°êµ¬ì§„ì€",
            "ì— ë”°ë¥´ë©´",
            "ì— ì˜í•˜ë©´",
            "ì¶œì²˜:",
            "ìë£Œ:"
        ]
        
        # 1. ì œëª©ì´ë‚˜ ìŠ¤ë‹ˆí«ì—ì„œ í•µì‹¬ ìˆ˜ì¹˜ë‚˜ ë¬¸êµ¬ ì¶”ì¶œ
        numbers = re.findall(r'\d+(?:\.\d+)?%?', snippet)
        key_phrases = re.findall(r'[^\s,]+\s[^\s,]+\s[^\s,]+', snippet)
        
        # 2. ì¸ìš© íŒ¨í„´ê³¼ í•¨ê»˜ ìˆ˜ì¹˜/ë¬¸êµ¬ê°€ ì‚¬ìš©ë˜ì—ˆëŠ”ì§€ í™•ì¸
        for pattern in citation_patterns:
            for number in numbers:
                if f"{pattern} {number}" in content_lower:
                    return True
            for phrase in key_phrases:
                if f"{pattern} {phrase}" in content_lower:
                    return True
        
        # 3. ì œëª©ì´ë‚˜ ìŠ¤ë‹ˆí«ì˜ í•µì‹¬ ë‚´ìš©ì´ ë³¸ë¬¸ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        if (title and title in content_lower) or (snippet and snippet in content_lower):
            return True
        
        return False

    def add_references(self, content: str, research_data: Dict) -> str:
        """ì½˜í…ì¸ ì— ì‚¬ìš©ëœ ì¶œì²˜ì™€ ëª¨ë“  ê´€ë ¨ ìë£Œ ì¶”ê°€"""
        used_sources = []
        all_sources = []
        
        # ëª¨ë“  ì†ŒìŠ¤ ìˆ˜ì§‘ ë° ë¶„ë¥˜
        for source_type, items in research_data.items():
            if not isinstance(items, list):
                continue
                
            for item in items:
                if not isinstance(item, dict):
                    continue
                    
                title = item.get('title', '')
                url = item.get('url', '')
                snippet = item.get('snippet', '').lower()
                date = item.get('date', '')
                
                if not url:  # URLì´ ì—†ëŠ” ê²½ìš° ê±´ë„ˆë›°ê¸°
                    continue
                
                source_info = {
                    'type': source_type,
                    'title': title,
                    'url': url,
                    'date': date,
                    'snippet': snippet
                }
                
                # ë³¸ë¬¸ì—ì„œ ì‚¬ìš©ëœ ì†ŒìŠ¤ í™•ì¸ - ìƒˆë¡œìš´ ë§¤ì¹­ ë¡œì§ ì‚¬ìš©
                if self._find_citation_in_content(content, source_info):
                    used_sources.append(source_info)
                
                all_sources.append(source_info)
        
        # ì°¸ê³ ìë£Œ ì„¹ì…˜ ì¶”ê°€
        content += "\n\n---\n## ì°¸ê³ ìë£Œ\n"
        
        # ë³¸ë¬¸ì—ì„œ ì‚¬ìš©ëœ ìë£Œ
        if used_sources:
            content += "\n### ğŸ“š ë³¸ë¬¸ì—ì„œ ì¸ìš©ëœ ìë£Œ\n"
            for source in used_sources:
                if source['date']:
                    content += f"- [{source['title']}]({source['url']}) ({source['date']})\n"
                else:
                    content += f"- [{source['title']}]({source['url']})\n"
        
        # ëª¨ë“  ê´€ë ¨ ìë£Œ
        content += "\n### ğŸ” ì¶”ê°€ ì°¸ê³ ìë£Œ\n"
        
        # ë‰´ìŠ¤ ìë£Œ
        content += "\n#### ğŸ“° ë‰´ìŠ¤ ìë£Œ\n"
        news_sources = [s for s in all_sources if s['type'] == 'news']
        for source in news_sources:
            content += f"- [{source['title']}]({source['url']})"
            if source['date']:
                content += f" ({source['date']})"
            content += "\n"
        
        # í•™ìˆ  ìë£Œ
        content += "\n#### ğŸ“š í•™ìˆ /ì—°êµ¬ ìë£Œ\n"
        academic_sources = [s for s in all_sources if s['type'] == 'academic']
        for source in academic_sources:
            content += f"- [{source['title']}]({source['url']})\n"
        
        # Perplexity ê²€ìƒ‰ ê²°ê³¼
        if any(s for s in all_sources if s['type'] == 'perplexity'):
            content += "\n#### ğŸ” ì¶”ê°€ ê²€ìƒ‰ ê²°ê³¼\n"
            perplexity_sources = [s for s in all_sources if s['type'] == 'perplexity']
            for source in perplexity_sources:
                content += f"- [{source['title']}]({source['url']})\n"
        
        return content

    def count_chars(self, text: str) -> dict:
        """ê¸€ììˆ˜ ë¶„ì„"""
        text_without_spaces = text.replace(" ", "")
        count = len(text_without_spaces)
        return {
            "count": count,
            "is_valid": 1700 <= count <= 2000
        }

    def analyze_morphemes(self, text: str, keyword: str = None, custom_morphemes: List[str] = None) -> dict:
        """í˜•íƒœì†Œ ë¶„ì„ ë° ì¶œí˜„ íšŸìˆ˜ ê²€ì¦"""
        if not keyword:
            return {}

        if self.okt is None:
            raise RuntimeError("í˜•íƒœì†Œ ë¶„ì„ê¸°(Okt)ê°€ ì •ìƒì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        # ì •í™•í•œ ì¹´ìš´íŒ…ì„ ìœ„í•œ ì „ì²˜ë¦¬
        text = re.sub(r'<[^>]+>', '', text)  # HTML íƒœê·¸ ì œê±°
        text = re.sub(r'[^\w\sê°€-í£]', ' ', text)  # íŠ¹ìˆ˜ë¬¸ì ì²˜ë¦¬ (í•œê¸€ í¬í•¨)
        
        # ì •í™•í•œ ë‹¨ì–´ ë‹¨ìœ„ ì¹´ìš´íŒ… í•¨ìˆ˜
        def count_exact_word(word, text):
            pattern = rf'\b{word}\b|\b{word}(?=[\s.,!?])|(?<=[\s.,!?]){word}\b'
            return len(re.findall(pattern, text))
        
        # í‚¤ì›Œë“œì™€ í˜•íƒœì†Œ ì¶œí˜„ íšŸìˆ˜ ê³„ì‚°
        keyword_count = count_exact_word(keyword, text)
        morphemes = self.okt.morphs(keyword)
        
        # ì‚¬ìš©ì ì§€ì • í˜•íƒœì†Œ ì¶”ê°€
        if custom_morphemes:
            morphemes.extend(custom_morphemes)
        morphemes = list(set(morphemes))  # ì¤‘ë³µ ì œê±°

        analysis = {
            "is_valid": True,
            "morpheme_analysis": {},
            "needs_optimization": False
        }

        # í‚¤ì›Œë“œ ë¶„ì„
        analysis["morpheme_analysis"][keyword] = {
            "count": keyword_count,
            "is_valid": 17 <= keyword_count <= 20,
            "status": "ì ì •" if 17 <= keyword_count <= 20 else "ê³¼ë‹¤" if keyword_count > 20 else "ë¶€ì¡±"
        }

        # í˜•íƒœì†Œ ë¶„ì„
        for morpheme in morphemes:
            count = count_exact_word(morpheme, text)
            is_valid = 17 <= count <= 20
            
            if not is_valid:
                analysis["is_valid"] = False
                analysis["needs_optimization"] = True

            analysis["morpheme_analysis"][morpheme] = {
                "count": count,
                "is_valid": is_valid,
                "status": "ì ì •" if is_valid else "ê³¼ë‹¤" if count > 20 else "ë¶€ì¡±"
            }

        return analysis

    def analyze_reference(self, reference_url: str) -> Dict:
        """ì°¸ê³  ë¸”ë¡œê·¸ ë¶„ì„"""
        try:
            reference_prompt = f"""
            ë‹¤ìŒ URLì˜ ì½˜í…ì¸ ë¥¼ ë¶„ì„í•˜ì—¬ ì£¼ìš” íŠ¹ì§•ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”:
            URL: {reference_url}
            
            ë¶„ì„ í•­ëª©:
            1. ê¸€ì˜ êµ¬ì¡°ì™€ íë¦„
            2. í•µì‹¬ í‚¤ì›Œë“œ ì‚¬ìš© ë°©ì‹
            3. ì£¼ìš” ë°ì´í„°ì™€ í†µê³„
            4. ì„¤ë“ë ¥ ìˆëŠ” ë…¼ë¦¬ ì „ê°œ ë°©ì‹
            5. ë…ì ê³µê°ëŒ€ í˜•ì„± ì „ëµ
            6. CTA(Call-to-Action) ë°©ì‹
            
            ê° í•­ëª©ë³„ë¡œ êµ¬ì²´ì ì¸ ì˜ˆì‹œì™€ í•¨ê»˜ ì„¤ëª…í•´ì£¼ì„¸ìš”.
            """
            
            response = self.claude.invoke(reference_prompt)
            
            # Claudeì˜ ì‘ë‹µ ì²˜ë¦¬
            if hasattr(response, 'content'):
                analysis = response.content
            else:
                analysis = str(response)
                
            return {
                'url': reference_url,
                'analysis': analysis
            }
            
        except Exception as e:
            logger.error(f"ì°¸ê³  ë¸”ë¡œê·¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return {
                'url': reference_url,
                'error': str(e)
            }
        

    def _create_content_prompt(self, data: Dict) -> str:
        """ìƒì„¸í•œ ì¡°ê±´ì„ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        keyword = data["keyword"]
        morphemes = self.okt.morphs(keyword)
        
        # ì•ˆì „í•œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        target_audience = data.get('target_audience', {})
        business_info = data.get('business_info', {})
        research_data = data.get('research_data', {})
        reference_analysis = data.get('reference_analysis', {}).get('analysis', '')
        
        # ì—°êµ¬ ìë£Œ í¬ë§·íŒ… (ìµœëŒ€ 2ê°œì”©ë§Œ ì‚¬ìš©)
        research_text = ""
        if isinstance(research_data, dict):
            news = research_data.get('news', [])[:2]
            academic = research_data.get('academic', [])[:2]
            
            if news:
                research_text += "ğŸ“° ë‰´ìŠ¤ ìë£Œ:\n"
                for item in news:
                    research_text += f"- {item.get('title', '')}: {item.get('snippet', '')}\n"
            
            if academic:
                research_text += "\nğŸ“š í•™ìˆ  ìë£Œ:\n"
                for item in academic:
                    research_text += f"- {item.get('title', '')}: {item.get('snippet', '')}\n"

        statistics_text = ""
        if isinstance(research_data.get('statistics'), list):
            statistics_text = "\nğŸ’¡ í™œìš© ê°€ëŠ¥í•œ í†µê³„ ìë£Œ:\n"
            for stat in research_data['statistics']:
                statistics_text += f"- {stat['context']} (ì¶œì²˜: {stat['source_title']})\n"

        prompt = f"""
        ë‹¤ìŒ ì¡°ê±´ì„ ì¤€ìˆ˜í•˜ì—¬ ì „ë¬¸ì„± ìˆê³  ì¹œê·¼í•œ ë¸”ë¡œê·¸ ê¸€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:

        í•„ìˆ˜ í™œìš© ìë£Œ:
        {research_text}
        
        í†µê³„ ìë£Œ (ë°˜ë“œì‹œ 1ê°œ ì´ìƒ í™œìš©):
        {statistics_text}

        1. ê¸€ì˜ êµ¬ì¡°ì™€ í˜•ì‹
        - ì „ì²´ êµ¬ì¡°: ì„œë¡ (20%) - ë³¸ë¡ (60%) - ê²°ë¡ (20%)
        - ê° ì†Œì œëª©ì€ ### ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ í‘œì‹œ
        - ì†Œì œëª© êµ¬ì„±:
        ### {data['subtopics'][0] if len(data['subtopics']) > 0 else 'ì†Œì œëª©1'}
        ### {data['subtopics'][1] if len(data['subtopics']) > 1 else 'ì†Œì œëª©2'}
        ### {data['subtopics'][2] if len(data['subtopics']) > 2 else 'ì†Œì œëª©3'}
        ### {data['subtopics'][3] if len(data['subtopics']) > 3 else 'ì†Œì œëª©4'}
        - ì „ì²´ ê¸¸ì´: 1700-2000ì (ê³µë°± ì œì™¸)

        2. [í•„ìˆ˜] ì„œë¡  ì‘ì„± ê°€ì´ë“œ
        ë°˜ë“œì‹œ ë‹¤ìŒ êµ¬ì¡°ë¡œ ì„œë¡ ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:
        1) ë…ìì˜ ê³ ë¯¼/ë¬¸ì œ ê³µê° (ë°˜ë“œì‹œ ìµœì‹  í†µê³„ë‚˜ ì—°êµ¬ ê²°ê³¼ ì¸ìš©)
        - ìˆ˜ì§‘ëœ í†µê³„ìë£Œë‚˜ ì—°êµ¬ê²°ê³¼ë¥¼ í™œìš©í•˜ì—¬ ë¬¸ì œì˜ ì‹¬ê°ì„±ì´ë‚˜ ì¤‘ìš”ì„± ê°•ì¡°
        - "ìµœê·¼ ì—°êµ¬ì— ë”°ë¥´ë©´..." ë˜ëŠ” "...ì˜ í†µê³„ì— ì˜í•˜ë©´..."ê³¼ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ì‹œì‘
        - "{keyword}ì— ëŒ€í•´ ê³ ë¯¼ì´ ë§ìœ¼ì‹ ê°€ìš”?"
        - íƒ€ê²Ÿ ë…ìì˜ êµ¬ì²´ì ì¸ ì–´ë ¤ì›€ ì–¸ê¸‰: {', '.join(target_audience.get('pain_points', []))}
        
        2) ì „ë¬¸ê°€ë¡œì„œì˜ í•´ê²°ì±… ì œì‹œ
        - "ì´ëŸ° ë¬¸ì œëŠ” {keyword}ë§Œ ì˜ ì•Œê³ ìˆì–´ë„ í•´ê²°ë˜ëŠ” ë¬¸ì œì…ë‹ˆë‹¤"
        - "{business_info.get('name', '')}ê°€ {business_info.get('expertise', '')}ì„ ë°”íƒ•ìœ¼ë¡œ í•´ê²°í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤"
        
        3) ë…ì ê´€ì‹¬ ìœ ë„
        - "ì´ ê¸€ì—ì„œëŠ” êµ¬ì²´ì ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ ë‚´ìš©ì„ ë‹¤ë£¹ë‹ˆë‹¤" í›„ ì†Œì œëª© ë¯¸ë¦¬ë³´ê¸°
        - "5ë¶„ë§Œ íˆ¬ìí•˜ì‹œë©´ {keyword}ì— ëŒ€í•œ ëª¨ë“  ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤"

        3. ê¸€ì“°ê¸° ìŠ¤íƒ€ì¼
        - ì „ë¬¸ê°€ì˜ ì§€ì‹ì„ ì‰½ê²Œ ì„¤ëª…í•˜ë“¯ì´ í¸ì•ˆí•œ í†¤ ìœ ì§€
        - ê° ë¬¸ë‹¨ì€ ìì—°ìŠ¤ëŸ½ê²Œ ë‹¤ìŒ ë¬¸ë‹¨ìœ¼ë¡œ ì—°ê²°
        - ìŠ¤í† ë¦¬í…”ë§ ìš”ì†Œ í™œìš©
        - ì‹¤ì œ ì‚¬ë¡€ë‚˜ ë¹„ìœ ë¥¼ í†µí•´ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…

        4. í•µì‹¬ í‚¤ì›Œë“œ í™œìš©
        - ì£¼ í‚¤ì›Œë“œ: {keyword}
        - í˜•íƒœì†Œ: {', '.join(morphemes)}
        - ê° í‚¤ì›Œë“œì™€ í˜•íƒœì†Œ 17-20íšŒ ìì—°ìŠ¤ëŸ½ê²Œ ì‚¬ìš©
        
        5. [í•„ìˆ˜] ì°¸ê³  ìë£Œ í™œìš©
        - ê° ì†Œì œëª© ì„¹ì…˜ë§ˆë‹¤ ìµœì†Œ 1ê°œ ì´ìƒì˜ ê´€ë ¨ í†µê³„/ì—°êµ¬ ìë£Œ ë°˜ë“œì‹œ ì¸ìš©
        - ì¸ìš©í•  ë•ŒëŠ” "~ì— ë”°ë¥´ë©´", "~ì˜ ì—°êµ¬ ê²°ê³¼", "~ì˜ í†µê³„ì— ì˜í•˜ë©´" ë“± ëª…í™•í•œ í‘œí˜„ ì‚¬ìš©
        - ëª¨ë“  í†µê³„ì™€ ìˆ˜ì¹˜ëŠ” ì¶œì²˜ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ëª…ì‹œ (ì˜ˆ: "2024ë…„ OOì—°êµ¬ì†Œì˜ ì¡°ì‚¬ì— ë”°ë¥´ë©´...")
        - ê°€ëŠ¥í•œ ìµœì‹  ìë£Œë¥¼ ìš°ì„ ì ìœ¼ë¡œ í™œìš©
        - í†µê³„ë‚˜ ìˆ˜ì¹˜ë¥¼ ì¸ìš©í•  ë•ŒëŠ” ê·¸ ì˜ë¯¸ë‚˜ ì‹œì‚¬ì ë„ í•¨ê»˜ ì„¤ëª…

        6. ë³¸ë¡  ì‘ì„± ê°€ì´ë“œ
        - ê° ì†Œì œëª©ë§ˆë‹¤ í•µì‹¬ ì£¼ì œ í•œ ì¤„ ìš”ì•½ìœ¼ë¡œ ì‹œì‘
        - ì´ë¡  â†’ ì‚¬ë¡€ â†’ ì‹¤ì²œ ë°©ë²• ìˆœìœ¼ë¡œ êµ¬ì„±
        - ì°¸ê³  ìë£Œì˜ í†µê³„ë‚˜ ì—°êµ¬ ê²°ê³¼ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ì¸ìš©
        - ì „ë¬¸ì  ë‚´ìš©ë„ ì‰½ê²Œ í’€ì–´ì„œ ì„¤ëª…
        - ê° ì„¹ì…˜ ëì—ì„œ ë‹¤ìŒ ì„¹ì…˜ìœ¼ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ì—°ê²°

        7. ê²°ë¡  ì‘ì„± ê°€ì´ë“œ
        - ë³¸ë¡  ë‚´ìš© ìš”ì•½
        - ì‹¤ì²œ ê°€ëŠ¥í•œ ë‹¤ìŒ ë‹¨ê³„ ì œì‹œ
        - "{business_info.get('name', '')}ê°€ ë„ì™€ë“œë¦´ ìˆ˜ ìˆë‹¤"ëŠ” ë©”ì‹œì§€
        - ë…ìì™€ì˜ ìƒí˜¸ì‘ìš© ìœ ë„

        8. ì°¸ê³  ë¸”ë¡œê·¸ ë¶„ì„ ê²°ê³¼ ë°˜ì˜:
        {reference_analysis}

        ìœ„ ì¡°ê±´ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì „ë¬¸ì„±ê³¼ ì¹œê·¼í•¨ì´ ì¡°í™”ëœ,
        ì½ê¸° ì‰½ê³  ì‹¤ìš©ì ì¸ ë¸”ë¡œê·¸ ê¸€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.
        íŠ¹íˆ íƒ€ê²Ÿ ë…ì({target_audience.get('primary', '')})ì˜ ì–´ë ¤ì›€ì„ í•´ê²°í•˜ëŠ”ë° ì´ˆì ì„ ë§ì¶°ì£¼ì„¸ìš”.
        """
        
        return prompt

    def _create_optimization_prompt(self, content: str, data: Dict) -> str:
        keyword = data['keyword']
        morphemes = self.okt.morphs(keyword)
        
        analysis = self.analyze_morphemes(content, keyword)
        current_counts = {word: info["count"] for word, info in analysis["morpheme_analysis"].items()}
        
        # ë™ì ìœ¼ë¡œ ì˜ˆì‹œ ìƒì„±
        example_instructions = f"""
        1. ë™ì˜ì–´/ìœ ì˜ì–´ë¡œ ëŒ€ì²´:
        - '{keyword}' ë˜ëŠ” ê° í˜•íƒœì†Œë¥¼ ìì—°ìŠ¤ëŸ¬ìš´ ë™ì˜ì–´/ìœ ì˜ì–´ë¡œ ëŒ€ì²´
        - í•´ë‹¹ ë¶„ì•¼ì˜ ì „ë¬¸ìš©ì–´ì™€ ì¼ë°˜ì ì¸ í‘œí˜„ì„ ì ì ˆíˆ í˜¼ìš©
        
        2. ë¬¸ë§¥ìƒ ìì—°ìŠ¤ëŸ¬ìš´ ìƒëµ:
        - "{keyword}ê°€ ì¤‘ìš”í•©ë‹ˆë‹¤" â†’ "ì¤‘ìš”í•©ë‹ˆë‹¤"
        - "{keyword}ë¥¼ ì‚´í´ë³´ë©´" â†’ "ì‚´í´ë³´ë©´"
        
        3. ì§€ì‹œì–´ë¡œ ëŒ€ì²´:
        - "{keyword}ëŠ”" â†’ "ì´ê²ƒì€"
        - "{keyword}ì˜ ê²½ìš°" â†’ "ì´ ê²½ìš°"
        - "ì´", "ì´ê²ƒ", "í•´ë‹¹", "ì´ëŸ¬í•œ" ë“±ì˜ ì§€ì‹œì–´ í™œìš©
        """

        return f"""
        ë‹¤ìŒ ë¸”ë¡œê·¸ ê¸€ì„ ìµœì í™”í•´ì£¼ì„¸ìš”. ë‹¤ìŒì˜ ì¶œí˜„ íšŸìˆ˜ ì œí•œì„ ë°˜ë“œì‹œ ì§€ì¼œì£¼ì„¸ìš”:

        ğŸ¯ ëª©í‘œ:
        1. í‚¤ì›Œë“œ '{keyword}': ì •í™•íˆ 17-20íšŒ ì‚¬ìš©
        2. ê° í˜•íƒœì†Œ({', '.join(morphemes)}): ì •í™•íˆ 17-20íšŒ ì‚¬ìš©
        
        ğŸ“Š í˜„ì¬ ìƒíƒœ:
        {chr(10).join([f"- '{word}': {count}íšŒ" for word, count in current_counts.items()])}

        âœ‚ï¸ ê³¼ë‹¤ ì‚¬ìš©ëœ ë‹¨ì–´ ìµœì í™” ë°©ë²• (ìš°ì„ ìˆœìœ„ ìˆœ):
        {example_instructions}

        âš ï¸ ì¤‘ìš”:
        - ê° í˜•íƒœì†Œì™€ í‚¤ì›Œë“œê°€ ì •í™•íˆ 17-20íšŒ ë²”ìœ„ ë‚´ì—ì„œ ì‚¬ìš©ë˜ì–´ì•¼ í•¨
        - ctrl+fë¡œ ê²€ìƒ‰í–ˆì„ ë•Œì˜ íšŸìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•¨
        - ì „ì²´ ë¬¸ë§¥ì˜ ìì—°ìŠ¤ëŸ¬ì›€ì„ ë°˜ë“œì‹œ ìœ ì§€
        - ì „ë¬¸ì„±ê³¼ ê°€ë…ì„±ì˜ ê· í˜• ìœ ì§€
        - ë™ì˜ì–´/ìœ ì˜ì–´ ì‚¬ìš©ì„ ìš°ì„ ìœ¼ë¡œ í•˜ê³ , ìì—°ìŠ¤ëŸ¬ìš´ ê²½ìš°ì—ë§Œ ìƒëµì´ë‚˜ ì§€ì‹œì–´ ì‚¬ìš©

        ì›ë¬¸:
        {content}

        ìœ„ ì§€ì¹¨ì— ë”°ë¼ ê³¼ë‹¤ ì‚¬ìš©ëœ í˜•íƒœì†Œë“¤ì„ ìµœì í™”í•˜ì—¬ ëª¨ë“  í˜•íƒœì†Œê°€ 17-20íšŒ ë²”ìœ„ ë‚´ì— ë“¤ë„ë¡ 
        ìì—°ìŠ¤ëŸ½ê²Œ ìˆ˜ì •í•´ì£¼ì„¸ìš”. ì „ë¬¸ì„±ì€ ìœ ì§€í•˜ë˜ ì½ê¸° ì‰½ê²Œ ìˆ˜ì •í•´ì£¼ì„¸ìš”.
        """

    def _needs_optimization(self, content: str, keyword: str) -> bool:
        """ìµœì í™” í•„ìš” ì—¬ë¶€ í™•ì¸"""
        # ê¸€ììˆ˜ í™•ì¸
        text_length = len(content.replace(" ", ""))
        if text_length < 1700 or text_length > 2000:
            return True
            
        # í‚¤ì›Œë“œ ì¶œí˜„ ë¹ˆë„ í™•ì¸
        keyword_count = content.lower().count(keyword.lower())
        if keyword_count < 15 or keyword_count > 20:
            return True
            
        # ë¬¸ì¥ ê¸¸ì´ í™•ì¸ (í•œ ë¬¸ì¥ì´ ë„ˆë¬´ ê¸¸ì§€ ì•Šì€ì§€)
        sentences = content.split('.')
        if any(len(sent.strip()) > 200 for sent in sentences):
            return True
            
        return False

    def check_quality(self, content: str, keyword: str) -> Dict:
        """ì½˜í…ì¸  í’ˆì§ˆ ê²€ì‚¬"""
        return {
            'length': len(content.replace(" ", "")),
            'readability': self._check_readability(content),
            'structure': self._check_structure(content),
            'keyword_density': self._check_keyword_density(content, keyword)
        }
        
    def _check_readability(self, content: str) -> Dict:
        """ê°€ë…ì„± ê²€ì‚¬"""
        sentences = content.split('.')
        return {
            'avg_sentence_length': sum(len(s.strip()) for s in sentences) / len(sentences),
            'long_sentences': sum(1 for s in sentences if len(s.strip()) > 100)
        }
        
    def _check_structure(self, content: str) -> Dict:
        """êµ¬ì¡° ê²€ì‚¬"""
        sections = content.split('\n\n')
        return {
            'total_sections': len(sections),
            'has_intro': bool(sections[0]),
            'has_conclusion': bool(sections[-1])
        }
        
    def _check_keyword_density(self, content: str, keyword: str) -> Dict:
        """í‚¤ì›Œë“œ ë°€ë„ ê²€ì‚¬"""
        total_words = len(content.split())
        keyword_count = content.lower().count(keyword.lower())
        return {
            'keyword_count': keyword_count,
            'density': keyword_count / total_words if total_words > 0 else 0
        }

    def collect_research(self, keyword: str, subtopics: List[str]) -> Dict:
        """í‚¤ì›Œë“œì™€ ì†Œì œëª© ê´€ë ¨ ì—°êµ¬ ìë£Œ ìˆ˜ì§‘"""
        all_results = {
            'news': [],
            'academic': [],
            'perplexity': [],
            'statistics': []
        }
        
        # 1. í‚¤ì›Œë“œ ê´€ë ¨ ìë£Œ ìˆ˜ì§‘
        search_queries = [
            f"{keyword} í†µê³„",
            f"{keyword} ì—°êµ¬ê²°ê³¼",
            f"{keyword} ìµœì‹  ë™í–¥",
            f"{keyword} ì‹œì¥ í˜„í™©",
            f"{keyword} íŠ¸ë Œë“œ"
        ]
        
        # 2. ì†Œì œëª© ê´€ë ¨ ìë£Œ ìˆ˜ì§‘
        for subtopic in subtopics:
            search_queries.extend([
                f"{keyword} {subtopic}",
                f"{keyword} {subtopic} í†µê³„",
                f"{keyword} {subtopic} ì—°êµ¬"
            ])
        
        for query in search_queries:
            # ë‰´ìŠ¤ ê²€ìƒ‰ (ìµœì‹ ìˆœìœ¼ë¡œ ì •ë ¬)
            news_results = self._get_news_from_serper(query)
            all_results['news'].extend(news_results)
            
            # í•™ìˆ  ìë£Œ ê²€ìƒ‰
            academic_results = self._get_academic_from_tavily(query)
            all_results['academic'].extend(academic_results)
            
            # Perplexity ê²€ìƒ‰
            perplexity_results = self._get_perplexity_search(query)
            all_results['perplexity'].extend(perplexity_results)
        
        # 3. í†µê³„ ë°ì´í„° ì¶”ì¶œ (ëª¨ë“  ìˆ˜ì§‘ëœ ìë£Œì—ì„œ)
        for category in ['news', 'academic', 'perplexity']:
            for item in all_results[category]:
                statistics = self._extract_statistics_from_text(
                    item.get('title', '') + ' ' + item.get('snippet', '')
                )
                if statistics:
                    for stat in statistics:
                        stat['source_url'] = item.get('url', '')
                        stat['source_title'] = item.get('title', '')
                        # ì¶œì²˜ì™€ ë‚ ì§œ ì •ë³´ ì¶”ê°€
                        stat['source'] = item.get('source', '')
                        stat['date'] = item.get('date', '')
                    all_results['statistics'].extend(statistics)
        
        # 4. ì¤‘ë³µ ì œê±° ë° ìµœì‹ ìˆœ ì •ë ¬
        for category in all_results:
            all_results[category] = self._deduplicate_results(all_results[category])
            # ë‚ ì§œ ì •ë³´ê°€ ìˆëŠ” ê²½ìš° ìµœì‹ ìˆœ ì •ë ¬
            if category in ['news', 'statistics']:
                all_results[category].sort(
                    key=lambda x: x.get('date', ''), 
                    reverse=True
                )
        
        return all_results

    def _extract_statistics_from_text(self, text: str) -> List[Dict]:
        """í…ìŠ¤íŠ¸ì—ì„œ í†µê³„ ë°ì´í„° ì¶”ì¶œ"""
        statistics = []
        
        # ìˆ«ì/í¼ì„¼íŠ¸ íŒ¨í„´
        patterns = [
            r'(\d+(?:,\d{3})*(?:\.\d+)?)\s*(?:ëª…|ê°œ|ì›|ë‹¬ëŸ¬|ìœ„|ë°°|ì²œ|ë§Œ|ì–µ|%|í¼ì„¼íŠ¸)',  # í•œê¸€ ë‹¨ìœ„
            r'(\d+(?:,\d{3})*(?:\.\d+)?)\s*(?:people|users|dollars|percent|%)',  # ì˜ë¬¸ ë‹¨ìœ„
            r'(\d+(?:\.\d+)?)[%ï¼…]'  # í¼ì„¼íŠ¸ ê¸°í˜¸
        ]
        
        for pattern in patterns:
            matches = re.finditer(pattern, text)
            for match in matches:
                # í†µê³„ ë°ì´í„°ì˜ ì „í›„ ë¬¸ë§¥ ì¶”ì¶œ (ìµœëŒ€ 100ì)
                start = max(0, match.start() - 50)
                end = min(len(text), match.end() + 50)
                context = text[start:end].strip()
                
                statistics.append({
                    'value': match.group(0),
                    'context': context,
                    'pattern_type': 'numeric' if '%' not in match.group(0) else 'percentage'
                })
        
        return statistics

class BlogChainSystem:
    """ì „ì²´ ì‹œìŠ¤í…œ ê´€ë¦¬ í´ë˜ìŠ¤"""
    def __init__(self):
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        secrets = load_secrets()
        self.perplexity_api_key = secrets['api']['perplexity']
        self.serper_api_key = secrets['api']['serper']
        self.keyword_analyzer = KeywordAnalyzer()  
        self.research_collector = ResearchCollector() 
        self.content_generator = ContentGenerator()
        self.collected_references = []
        self.setup_chains()
        
    def handle_conversation(self):
        """ëŒ€í™” ìƒíƒœì— ë”°ë¥¸ í•¸ë“¤ëŸ¬ ì‹¤í–‰"""
        state = st.session_state.conversation_state
        
        if state.step == "keyword":
            self.handle_keyword_step()
        elif state.step == "seo_analysis":
            self.handle_seo_analysis_step()
        elif state.step == "subtopics":
            self.handle_subtopics_step()
        elif state.step == "target_audience":    
            self.handle_target_audience_step()
        elif state.step == "business_info":      
            self.handle_business_info_step()
        elif state.step == "morphemes":          
            self.handle_morphemes_step()
        elif state.step == "reference":          
            self.handle_reference_step()
        elif state.step == "content_creation":
            self.handle_content_creation_step()

    def handle_keyword_step(self):
        """í‚¤ì›Œë“œ ì…ë ¥ ë° ë¶„ì„ ë‹¨ê³„"""
        st.markdown("### ë¸”ë¡œê·¸ ì£¼ì œ ì„ ì •")
        keyword = st.text_input("ì–´ë–¤ ì£¼ì œë¡œ ë¸”ë¡œê·¸ë¥¼ ì‘ì„±í•˜ê³  ì‹¶ìœ¼ì‹ ê°€ìš”?")
        
        if keyword:
            with st.spinner("í‚¤ì›Œë“œ ë¶„ì„ ì¤‘..."):
                try:
                    # Perplexityë¡œ í‚¤ì›Œë“œ ë¶„ì„
                    analysis_result = self.keyword_analyzer.analyze_keyword(keyword)
                    
                    # ë¶„ì„ ê²°ê³¼ ì €ì¥
                    st.session_state.conversation_state.data['keyword'] = keyword
                    st.session_state.conversation_state.data['keyword_analysis'] = analysis_result
                    st.session_state.conversation_state.step = "seo_analysis"
                    st.rerun()
                    
                except Exception as e:
                    st.error(f"í‚¤ì›Œë“œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

    def handle_seo_analysis_step(self):
        """SEO ë¶„ì„ ê²°ê³¼ í‘œì‹œ ë° ì†Œì œëª© ì¶”ì²œ"""
        st.markdown("### ê²€ìƒ‰ íŠ¸ë Œë“œ ë¶„ì„ ê²°ê³¼")
        
        analysis_data = st.session_state.conversation_state.data.get('keyword_analysis', {})
        if analysis_data:
            # ë¶„ì„ ê²°ê³¼ í‘œì‹œ
            st.write("### ì£¼ìš” ê²€ìƒ‰ ì˜ë„")
            st.write(analysis_data.get('main_intent', ''))
            
            st.write("### ê²€ìƒ‰ìê°€ ì–»ê³ ì í•˜ëŠ” ì •ë³´")
            for info in analysis_data.get('info_needed', []):
                st.write(f"- {info}")
            
            st.write("### ê²€ìƒ‰ìì˜ ì£¼ìš” ì–´ë ¤ì›€")
            for pain in analysis_data.get('pain_points', []):
                st.write(f"- {pain}")
            
            # ì†Œì œëª© ì¶”ì²œ
            keyword = st.session_state.conversation_state.data['keyword']
            recommended_subtopics = self.keyword_analyzer.suggest_subtopics(keyword)
            
            # ì¶”ì²œ ì†Œì œëª©ì„ ì„¸ì…˜ ë°ì´í„°ì— ì €ì¥
            st.session_state.conversation_state.data['recommended_subtopics'] = recommended_subtopics
            
            st.markdown("### âœï¸ ì¶”ì²œ ì†Œì œëª©")
            for i, subtopic in enumerate(recommended_subtopics, 1):
                st.write(f"{i}. {subtopic}")
            
            col1, col2 = st.columns(2)
            with col1:
                if st.button("ì´ ì†Œì œëª©ë“¤ë¡œ ì§„í–‰í•˜ê¸°"):
                    st.session_state.conversation_state.data['subtopics'] = recommended_subtopics.copy()
                    st.session_state.conversation_state.step = "target_audience"
                    st.rerun()
            with col2:
                if st.button("ì†Œì œëª© ì§ì ‘ ì…ë ¥í•˜ê¸°"):
                    # ì†Œì œëª© ì§ì ‘ ì…ë ¥ ë‹¨ê³„ë¡œ ì´ë™í•  ë•Œë„ ì¶”ì²œ ì†Œì œëª©ì„ ìœ ì§€
                    st.session_state.conversation_state.step = "subtopics"
                    st.rerun()

    def handle_business_info_step(self):
        """ì‚¬ì—…ì ì •ë³´ ì…ë ¥"""
        st.markdown("### ì‚¬ì—…ì ì •ë³´ ì…ë ¥")
        
        business_name = st.text_input("ìƒí˜¸ëª…")
        expertise = st.text_area("ì „ë¬¸ì„±/ê²½ë ¥ ì‚¬í•­")
        
        if business_name and expertise and st.button("ë‹¤ìŒ ë‹¨ê³„ë¡œ"):
            # ì‚¬ì—…ì ì •ë³´ ì €ì¥
            st.session_state.conversation_state.data['business_info'] = {
                'name': business_name,
                'expertise': expertise
            }
            
            # ì—°êµ¬ ìë£Œ ìˆ˜ì§‘ ì‹œì‘
            with st.spinner("ê´€ë ¨ëœ ê¸°ì‚¬ë‚˜ í†µê³„ìë£Œë¥¼ ìˆ˜ì§‘í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                try:
                    keyword = st.session_state.conversation_state.data['keyword']
                    # recommended_subtopicsë‚˜ subtopics ì¤‘ ì¡´ì¬í•˜ëŠ” ê²ƒ ì‚¬ìš©
                    subtopics = (st.session_state.conversation_state.data.get('subtopics', []) or 
                                st.session_state.conversation_state.data.get('recommended_subtopics', []))
                    
                    if subtopics:
                        research_data = self.research_collector.collect_research(keyword, subtopics)
                        st.session_state.conversation_state.data['research_data'] = research_data
                    else:
                        st.warning("ì†Œì œëª© ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        return
                except Exception as e:
                    st.error(f"ì—°êµ¬ ìë£Œ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                    return
            
            # ë‹¤ìŒ ë‹¨ê³„ë¡œ ì´ë™
            st.session_state.conversation_state.data['business_info'] = {
                "name": business_name,
                "expertise": expertise
            }
            st.session_state.conversation_state.step = "morphemes"  # ë³€ê²½
            st.rerun()

    def handle_reference_step(self):
        """ì°¸ê³  ë¸”ë¡œê·¸ ë¶„ì„ ë‹¨ê³„"""
        st.markdown("### ì°¸ê³  ë¸”ë¡œê·¸ ë¶„ì„")
        st.write("ì°¸ê³ í•˜ê³  ì‹¶ì€ ë¸”ë¡œê·¸ë‚˜ ê¸°ì‚¬ì˜ URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”. (ì„ íƒì‚¬í•­)")
        st.write("ğŸ’¡ ì°¸ê³ í•˜ê³  ì‹¶ì€ ë¸”ë¡œê·¸ì˜ ê¸€ì“°ê¸° ìŠ¤íƒ€ì¼ì„ ë¶„ì„í•˜ì—¬ ë°˜ì˜í•©ë‹ˆë‹¤.")
        
        reference_url = st.text_input("ì°¸ê³  URL")
        
        col1, col2 = st.columns(2)
        
        with col1:
            if st.button("ë¸”ë¡œê·¸ ì‘ì„±í•˜ê¸°"):
                # content_creation ë‹¨ê³„ë¡œ ì§ì ‘ ì´ë™
                st.session_state.conversation_state.step = "content_creation"
                st.rerun()
                
        with col2:
            if reference_url and st.button("ë¶„ì„í•˜ê¸°"):
                with st.spinner("ì°¸ê³  ë¸”ë¡œê·¸ë¥¼ ë¶„ì„ì¤‘ì…ë‹ˆë‹¤..."):
                    try:
                        # ë¸”ë¡œê·¸ ë¶„ì„ ìˆ˜í–‰
                        reference_analysis = self.content_generator.analyze_reference(reference_url)
                        
                        if 'error' not in reference_analysis:
                            st.success("ë¸”ë¡œê·¸ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
                            
                            # ë¶„ì„ ê²°ê³¼ í‘œì‹œ
                            with st.expander("âœï¸ ë¶„ì„ ê²°ê³¼ ë³´ê¸°"):
                                st.write("### ê¸€ì˜ êµ¬ì¡°ì™€ íë¦„")
                                st.write(reference_analysis['analysis'])
                            
                            # ë¶„ì„ ê²°ê³¼ ì €ì¥
                            st.session_state.conversation_state.data['reference_analysis'] = reference_analysis
                            
                            # ë¶„ì„ ì™„ë£Œ í›„ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì´ë™ ë²„íŠ¼ í‘œì‹œ
                            if st.button("ë‹¤ìŒ ë‹¨ê³„ë¡œ"):
                                st.session_state.conversation_state.step = "content_creation"
                                st.rerun()
                        else:
                            st.error(f"ë¸”ë¡œê·¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {reference_analysis['error']}")
                    
                    except Exception as e:
                        st.error(f"ë¸”ë¡œê·¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

        # ì´ì „ ë‹¨ê³„ë¡œ ëŒì•„ê°€ê¸° ë²„íŠ¼
        if st.button("ì´ì „ìœ¼ë¡œ"):
            st.session_state.conversation_state.step = "morphemes"
            st.rerun()

    def handle_target_audience_step(self):
        """íƒ€ê²Ÿ ë…ìì¸µ ì„¤ì • ë‹¨ê³„"""
        st.markdown("### íƒ€ê²Ÿ ë…ìì¸µ ì„¤ì •")
        
        # í‚¤ì›Œë“œ ë¶„ì„ ê²°ê³¼ì—ì„œ ì •ë³´ ì¶”ì¶œ
        analysis_data = st.session_state.conversation_state.data.get('keyword_analysis', {})
        raw_text = analysis_data.get('raw_text', '')
        default_target = ""
        default_pain_points = ""
        
        if raw_text:
            try:
                # ì£¼ìš” ê²€ìƒ‰ ì˜ë„ ì¶”ì¶œ
                if "1. ì£¼ìš” ê²€ìƒ‰ ì˜ë„:" in raw_text:
                    sections = raw_text.split("1. ì£¼ìš” ê²€ìƒ‰ ì˜ë„:")[1].split("2.")[0]
                    default_target = sections.strip()
                
                # ì–´ë ¤ì›€/ë¶ˆí¸í•¨ ì¶”ì¶œ
                if "3. ê²€ìƒ‰ìê°€ ê²ªê³  ìˆëŠ” ë¶ˆí¸í•¨ì´ë‚˜ ì–´ë ¤ì›€:" in raw_text:
                    difficulties_section = raw_text.split("3. ê²€ìƒ‰ìê°€ ê²ªê³  ìˆëŠ” ë¶ˆí¸í•¨ì´ë‚˜ ì–´ë ¤ì›€:")[1]
                    difficulties = []
                    for line in difficulties_section.split('\n'):
                        if line.strip().startswith('- '):
                            difficulties.append(line.replace('- ', '').strip())
                    default_pain_points = '\n'.join(difficulties)
            
            except Exception as e:
                logger.error(f"ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        
        primary_target = st.text_input(
            "ì£¼ìš” íƒ€ê²Ÿ ë…ìì¸µ",
            value=default_target,
            help="ì˜ˆì‹œ: ì†Œìƒê³µì¸, ìŠ¤íƒ€íŠ¸ì—… ëŒ€í‘œ, ë§ˆì¼€íŒ… ë‹´ë‹¹ì"
        )
        
        pain_points = st.text_area(
            "íƒ€ê²Ÿ ë…ìì¸µì´ ê²ªëŠ” ì–´ë ¤ì›€",
            value=default_pain_points,
            help="ì‰¼í‘œ(,)ë¡œ êµ¬ë¶„í•˜ì—¬ ì…ë ¥í•´ì£¼ì„¸ìš”."
        )
        
        additional_target = st.text_input(
            "ì¶”ê°€ íƒ€ê²Ÿ ë…ìì¸µ (ì„ íƒì‚¬í•­)",
            help="ì˜ˆì‹œ: ì˜ˆë¹„ ì°½ì—…ì, ë§ˆì¼€íŒ… í•™ìŠµì"
        )
        
        if primary_target and pain_points:
            if st.button("ë‹¤ìŒ ë‹¨ê³„ë¡œ"):
                target_info = {
                    "primary": primary_target,
                    "pain_points": pain_points.split('\n'),
                    "additional": additional_target if additional_target else None
                }
                st.session_state.conversation_state.data['target_audience'] = target_info
                st.session_state.conversation_state.step = "business_info"  # ë³€ê²½
                st.rerun()

    def handle_subtopics_step(self):
        """ì†Œì œëª© ì§ì ‘ ì…ë ¥ ë‹¨ê³„"""
        st.markdown("### ì†Œì œëª© ì„ ì •")
        
        # ì´ì „ ë‹¨ê³„ì—ì„œ ìƒì„±ëœ ì¶”ì²œ ì†Œì œëª© ê°€ì ¸ì˜¤ê¸°
        recommended_subtopics = []
        if 'recommended_subtopics' in st.session_state.conversation_state.data:
            recommended_subtopics = st.session_state.conversation_state.data['recommended_subtopics']
        
        st.write("ë¸”ë¡œê·¸ì˜ ì†Œì œëª©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”. (ì´ 4ê°œ)")
        subtopics = []
        
        # ì†Œì œëª© ì…ë ¥ í•„ë“œ - ê° í•„ë“œì— ì¶”ì²œ ì†Œì œëª©ì„ ì´ˆê¸°ê°’ìœ¼ë¡œ ì„¤ì •
        for i in range(4):
            # ì¶”ì²œ ì†Œì œëª©ì´ ìˆì„ ê²½ìš° í•´ë‹¹ ê°’ì„, ì—†ì„ ê²½ìš° ë¹ˆ ë¬¸ìì—´ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •
            default_value = recommended_subtopics[i] if i < len(recommended_subtopics) else ""
            
            # ê° ì†Œì œëª©ì˜ ë„ì›€ë§ í…ìŠ¤íŠ¸
            help_text = {
                0: "ê¸°ì´ˆ ê°œë…ì´ë‚˜ ì •ì˜ë¥¼ ë‹¤ë£¨ëŠ” ì†Œì œëª©",
                1: "ì£¼ìš” íŠ¹ì§•ì´ë‚˜ ì¥ì ì„ ì„¤ëª…í•˜ëŠ” ì†Œì œëª©",
                2: "ì‹¤ì œ í™œìš© ë°©ë²•ì´ë‚˜ íŒì„ ë‹¤ë£¨ëŠ” ì†Œì œëª©",
                3: "ì£¼ì˜ì‚¬í•­ì´ë‚˜ ì¶”ì²œì‚¬í•­ì„ ë‹¤ë£¨ëŠ” ì†Œì œëª©"
            }
            
            # keyë¥¼ uniqueí•˜ê²Œ ì„¤ì •í•˜ì—¬ ì´ˆê¸°ê°’ì´ ì œëŒ€ë¡œ ì ìš©ë˜ë„ë¡ í•¨
            input_key = f"subtopic_input_{i}_{default_value}"
            
            subtopic = st.text_input(
                f"ì†Œì œëª© {i+1}",
                value=default_value,
                help=help_text[i],
                key=input_key
            )
            
            if subtopic:
                subtopics.append(subtopic)
            else:
                subtopics.append(default_value)  # ë¹ˆ ê°’ì¼ ê²½ìš° ê¸°ë³¸ê°’ ì‚¬ìš©
        
        # ë²„íŠ¼ ë ˆì´ì•„ì›ƒ
        col1, col2 = st.columns(2)
        
        # ì†Œì œëª© í™•ì • ë²„íŠ¼
        with col1:
            if st.button("ì†Œì œëª© í™•ì •", use_container_width=True):
                if len(subtopics) == 4 and all(subtopics):
                    st.session_state.conversation_state.data['subtopics'] = subtopics
                    st.session_state.conversation_state.step = "target_audience"
                    st.rerun()
                else:
                    st.warning("4ê°œì˜ ì†Œì œëª©ì„ ëª¨ë‘ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        
        # ì´ì „ìœ¼ë¡œ ë²„íŠ¼
        with col2:
            if st.button("ì´ì „ìœ¼ë¡œ", use_container_width=True):
                st.session_state.conversation_state.step = "seo_analysis"
                st.rerun()

    def handle_morphemes_step(self):
        """í•µì‹¬ í˜•íƒœì†Œ ì„¤ì • ë‹¨ê³„"""
        st.markdown("### í•µì‹¬ í˜•íƒœì†Œ ì„¤ì •")
        
        # í˜„ì¬ í‚¤ì›Œë“œ ê°€ì ¸ì˜¤ê¸°
        keyword = st.session_state.conversation_state.data.get('keyword', '')
        
        # ê¸°ë³¸ í˜•íƒœì†Œ ë¶„ì„ ê²°ê³¼ í‘œì‹œ
        if keyword:
            default_morphemes = self.content_generator.okt.morphs(keyword)
            st.write(f"âš¡ '{keyword}'ì˜ ê¸°ë³¸ í˜•íƒœì†Œ:")
            for morpheme in default_morphemes:
                st.write(f"- {morpheme}")
        
        st.markdown("---")
        st.write("âœï¸ ë¸”ë¡œê·¸ì— ì¶”ê°€ë¡œ í¬í•¨í•˜ê³  ì‹¶ì€ í•µì‹¬ ë‹¨ì–´ë‚˜ í˜•íƒœì†Œê°€ ìˆë‹¤ë©´ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        st.write("ğŸ’¡ ê° í˜•íƒœì†ŒëŠ” ë„ì–´ì“°ê¸°ë¡œ êµ¬ë¶„í•˜ì—¬ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        
        morphemes_input = st.text_input(
            "ì¶”ê°€ í˜•íƒœì†Œ ì…ë ¥",
            help="ì˜ˆì‹œ: ìë™ì°¨ ì •ë¹„ ì•ˆì „ ì ê²€"
        )
        
        col1, col2 = st.columns(2)
        with col1:
            if st.button("ë‹¤ìŒ ë‹¨ê³„ë¡œ"):
                if morphemes_input:
                    additional_morphemes = [m.strip() for m in morphemes_input.split() if m.strip()]
                    morphemes = list(set(default_morphemes + additional_morphemes))
                    st.session_state.conversation_state.data['morphemes'] = morphemes
                else:
                    st.session_state.conversation_state.data['morphemes'] = default_morphemes
                st.session_state.conversation_state.step = "reference"
                st.rerun()
        with col2:
            if st.button("ì´ì „ìœ¼ë¡œ"):
                st.session_state.conversation_state.step = "business_info"
                st.rerun()

    def setup_chains(self):
        secrets = load_secrets()
        self.perplexity_api_key = secrets['api']['perplexity']
        
        self.llm = ChatPerplexity(
            model="sonar-pro",
            api_key=self.perplexity_api_key,
            temperature=0.7
        )
        
        # SEO ë¶„ì„ ì²´ì¸
        seo_template = ChatPromptTemplate.from_template("""ë‹¹ì‹ ì€ SEO ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ í‚¤ì›Œë“œë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:
        í‚¤ì›Œë“œ: {keyword}

        ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë¶„ì„ ê²°ê³¼ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:

        1. ì£¼ìš” ê²€ìƒ‰ ì˜ë„: 
        (2-3ë¬¸ì¥ìœ¼ë¡œ ì´ í‚¤ì›Œë“œë¥¼ ê²€ìƒ‰í•˜ëŠ” ì‚¬ëŒë“¤ì˜ ì£¼ìš” ì˜ë„ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”)

        2. ê²€ìƒ‰ìê°€ ì–»ê³ ì í•˜ëŠ” ì •ë³´:
        (ê°€ì¥ ì¤‘ìš”í•œ 3ê°€ì§€ë§Œ bullet pointë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”)
        - 
        - 
        - 

        3. ê²€ìƒ‰ìê°€ ê²ªê³  ìˆëŠ” ë¶ˆí¸í•¨ì´ë‚˜ ì–´ë ¤ì›€:
        (ê°€ì¥ ì¼ë°˜ì ì¸ 3ê°€ì§€ ì–´ë ¤ì›€ë§Œ bullet pointë¡œ ê°„ë‹¨í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”)
        - 
        - 
        - 

        ëª¨ë“  ë‚´ìš©ì€ ê°„ë‹¨ëª…ë£Œí•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.""")
        self.seo_chain = seo_template | self.llm

    def handle_content_creation_step(self):
        st.markdown("### ë¸”ë¡œê·¸ ì‘ì„±")
        state = st.session_state.conversation_state
        
        try:
            data = {
                "keyword": state.data.get('keyword', ''),
                "subtopics": state.data.get('subtopics', []) or state.data.get('recommended_subtopics', []),
                "target_audience": {
                    "primary": state.data.get('target_audience', {}).get('primary', ''),
                    "pain_points": state.data.get('target_audience', {}).get('pain_points', [])
                },
                "business_info": {
                    "name": state.data.get('business_info', {}).get('name', ''),
                    "expertise": state.data.get('business_info', {}).get('expertise', '')
                },
                "morphemes": state.data.get('morphemes', []),
                "reference_analysis": state.data.get('reference_analysis', {})
            }

            # ì—°êµ¬ ìë£Œ ìˆ˜ì§‘ (add_custom_references ëŒ€ì‹  collect_research ì§ì ‘ ì‚¬ìš©)
            research_data = self.research_collector.collect_research(
                data["keyword"], 
                data["subtopics"]
            )
            data["research_data"] = research_data

            progress_messages = [
                "âœ¨ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ê±°ë‚˜ ë¶ˆí¸í•˜ì‹ ì ì€ í¸í•˜ê²Œ ë¬¸ì˜ ë¶€íƒë“œë¦½ë‹ˆë‹¤."
            ]

            with st.spinner("ë¸”ë¡œê·¸ ì‘ì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤..."):
                for msg in progress_messages:
                    st.write(msg)
                    
                content_result = self.content_generator.generate_content(data)
                
                # ë¶„ì„ ë° ìµœì í™” ìˆ˜í–‰
                morpheme_analysis = self.content_generator.analyze_morphemes(
                    text=content_result, 
                    keyword=data["keyword"],
                    custom_morphemes=data.get('morphemes', [])
                )
                chars_analysis = self.content_generator.count_chars(content_result)
                
                if (not morpheme_analysis["is_valid"] or not chars_analysis["is_valid"]):
                    st.info("ìƒì„±ëœ ì½˜í…ì¸ ë¥¼ ìµœì í™”í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
                    content_result = self.content_generator.optimize_content(
                        content_result, 
                        data
                    )
                
                # ê²°ê³¼ í‘œì‹œ
                st.markdown("### ìµœì¢… ë¸”ë¡œê·¸ ë‚´ìš©")
                st.write(content_result)
                
                # ì°¸ê³ ìë£Œ ì„¹ì…˜ í‘œì‹œ
                if "### ì°¸ê³ ìë£Œ" in content_result:
                    with st.expander("ğŸ“š ì°¸ê³ í•œ ìë£Œ ë³´ê¸°"):
                        sources_section = content_result.split("### ì°¸ê³ ìë£Œ")[1].strip()
                        st.markdown(sources_section)
        
        except Exception as e:
            st.error(f"ì½˜í…ì¸  ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            logger.error(f"ë””ë²„ê¹…: {str(e)}", exc_info=True)

    def reset_conversation(self):
        """ëŒ€í™” ìƒíƒœ ì´ˆê¸°í™”"""
        st.session_state.conversation_state = ConversationState()

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    st.set_page_config(page_title="ğŸ§AI ë¸”ë¡œê·¸ ì¹˜íŠ¸í‚¤", layout="wide")
    
    # conversation_state ì´ˆê¸°í™”
    if 'conversation_state' not in st.session_state:
        st.session_state.conversation_state = ConversationState()
    
    # system ì´ˆê¸°í™”
    if 'system' not in st.session_state:
        st.session_state.system = BlogChainSystem()

    st.title("ğŸ§AI ë¸”ë¡œê·¸ ì¹˜íŠ¸í‚¤")
    
    # ì‚¬ì´ë“œë°”ì— ì§„í–‰ ìƒí™© í‘œì‹œ
    with st.sidebar:
        st.markdown("### ì§„í–‰ ìƒí™©")
        steps = {
            "keyword": "1ï¸âƒ£ í‚¤ì›Œë“œ ì„ ì •",
            "seo_analysis": "2ï¸âƒ£ ê²€ìƒ‰ íŠ¸ë Œë“œ ë¶„ì„",
            "subtopics": "3ï¸âƒ£ ì†Œì œëª© ì„ ì •",
            "target_audience": "4ï¸âƒ£ íƒ€ê²Ÿ ë…ì ì„¤ì •",
            "business_info": "5ï¸âƒ£ ì‚¬ì—…ì ì •ë³´",
            "morphemes": "6ï¸âƒ£ í•µì‹¬ í˜•íƒœì†Œ",
            "reference": "7ï¸âƒ£ ì°¸ê³  ë¸”ë¡œê·¸",
            "content_creation": "8ï¸âƒ£ ë¸”ë¡œê·¸ ì‘ì„±"
        }
        
        current_step = st.session_state.conversation_state.step
        for step, label in steps.items():
            if step == current_step:
                st.markdown(f"â†’ {label}")
            elif list(steps.keys()).index(step) < list(steps.keys()).index(current_step):
                st.markdown(f"âœ“ {label}")
            else:
                st.markdown(f"  {label}")
        
        if st.button("ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì‹œì‘"):
            st.session_state.system.reset_conversation()
            st.rerun()
    
    # ë©”ì¸ ì»¨í…ì¸  ì˜ì—­
    st.session_state.system.handle_conversation()

if __name__ == "__main__":
    main()  # ë©”ì¸ í•¨ìˆ˜ ì‹¤í–‰