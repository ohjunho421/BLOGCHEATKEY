import os
import tomli
import streamlit as st
import http.client
import json
import jpype
import logging
from langchain_community.chat_models import ChatPerplexity, ChatOpenAI
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.chains import LLMChain
from langchain_core.messages import AIMessage
from konlpy.tag import Okt
from dataclasses import dataclass
from typing import List, Dict, Optional
from collections import Counter
import re
import random

logging.basicConfig(level=logging.INFO)  # í•„ìš”í•  ê²½ìš° DEBUGë¡œ ë³€ê²½ ê°€ëŠ¥
logger = logging.getLogger(__name__)

def load_secrets():
    """ì‹œí¬ë¦¿ í‚¤ ë¡œë“œ í•¨ìˆ˜"""
    with open("secrets.toml", "rb") as f:
        return tomli.load(f)

SECRETS = load_secrets()
PERPLEXITY_API_KEY = SECRETS["api"]["perplexity"]
SERPER_API_KEY = SECRETS["api"]["serper"]

@dataclass
class ConversationState:
    """ëŒ€í™” ìƒíƒœë¥¼ ê´€ë¦¬í•˜ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    step: str = "keyword"
    data: Dict = None
    
    def __post_init__(self):
        if self.data is None:
            self.data = {}

class ContentAnalyzer:
    def __init__(self):
        """Perplexity API ë° í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”"""
        try:
            # ë§Œì•½ JVMì´ ì•„ì§ ì‹œì‘ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ìˆ˜ë™ìœ¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.
            if not jpype.isJVMStarted():
                jpype.startJVM()
            self.okt = Okt()  # Okt í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”
        except Exception as e:
            logger.error(f"âŒ í˜•íƒœì†Œ ë¶„ì„ê¸°(Okt) ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            self.okt = None  # Oktê°€ ì •ìƒì ìœ¼ë¡œ ë¡œë“œë˜ì§€ ì•Šì„ ê²½ìš° ëŒ€ë¹„

        self.llm = ChatPerplexity(
            model="sonar-pro",
            api_key=PERPLEXITY_API_KEY,
            temperature=0.7
        )
    
    def expand_existing_subtopics(self, content: str, keyword: str, subtopics: list) -> str:
        """ê¸€ììˆ˜ê°€ ë¶€ì¡±í•  ê²½ìš° ê¸°ì¡´ ì†Œì œëª© ë‚´ìš©ì„ í™•ì¥í•˜ì—¬ ë³´ì™„"""
        
        # ğŸ”¹ í˜„ì¬ ê¸€ì ìˆ˜ í™•ì¸ (ê³µë°± ì œì™¸)
        text_length = len(content.replace(" ", ""))

        if text_length >= 1500:
            return content  # âœ… ê¸€ììˆ˜ê°€ ì¶©ë¶„í•˜ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜

        expansion_prompt = f"""
        ë‹¤ìŒ ê¸€ì„ **1500~2000ì**ë¡œ í™•ì¥í•´ì£¼ì„¸ìš”.

        ğŸ”¹ **í•µì‹¬ í‚¤ì›Œë“œ:** {keyword}
        ğŸ”¹ **ì†Œì œëª© ëª©ë¡:** 
        {'\n'.join([f"- {sub}" for sub in subtopics])}

        âœ… **í™•ì¥ ê¸°ì¤€:**
        1. **ê¸°ì¡´ ì†Œì œëª© ë‚´ì—ì„œ ê²€ìƒ‰ìê°€ ê¶ê¸ˆí•´í•  ë‚´ìš©ì„ í™•ì¥** (ìƒˆ ì†Œì œëª© ì¶”ê°€ X)
        2. **ì‹¤ìš©ì ì¸ íŒ, ë°ì´í„° ê¸°ë°˜ ì •ë³´, ì‚¬ë¡€ ì—°êµ¬, FAQ ë“± ì¶”ê°€**
        3. **SEO ìµœì í™”:** `{keyword}`ì™€ ê´€ë ¨ëœ í•µì‹¬ ê°œë…ì„ ìœ ì§€
        4. **ë¬¸ì¥ì˜ íë¦„ì„ ìœ ì§€í•˜ë©´ì„œ ìì—°ìŠ¤ëŸ½ê²Œ í™•ì¥**
        5. **1500~2000ì ë‚´ë¡œ ìœ ì§€ (ë¶ˆí•„ìš”í•œ ë°˜ë³µ ê¸ˆì§€)**

        ğŸ“ **íŠ¹íˆ ì¶”ê°€í•  ë‚´ìš© ì˜ˆì‹œ:**  
        - `{subtopics[0]}`ì— ëŒ€í•œ ì‹¤ìš©ì ì¸ ì‚¬ë¡€  
        - `{subtopics[1]}`ê³¼ ê´€ë ¨ëœ ìµœì‹  ì—°êµ¬ ê²°ê³¼ ë˜ëŠ” í†µê³„  
        - `{subtopics[2]}`ì—ì„œ ìì£¼ ë¬»ëŠ” ì§ˆë¬¸ê³¼ ë‹µë³€  
        - `{subtopics[3]}`ì— ëŒ€í•œ ì „ë¬¸ê°€ ì¡°ì–¸ ë° íŒ  

        **ì›ë¬¸:**
        {content}
        """

        # ğŸ”¹ Perplexity API í˜¸ì¶œí•˜ì—¬ í™•ì¥ ìˆ˜í–‰
        response = self.llm.invoke(expansion_prompt)

        if isinstance(response, AIMessage):
            return response.content
        elif isinstance(response, dict) and 'content' in response:
            return response['content']
        elif isinstance(response, str):
            return response
        else:
            return str(response)


    def analyze_morphemes(self, text: str, keyword: str = None) -> dict:
        """í˜•íƒœì†Œ ë¶„ì„ ë° ì¶œí˜„ íšŸìˆ˜ ê²€ì¦"""
        if not keyword:
            return self._analyze_basic_morphemes(text)

        # ğŸ”¹ Oktê°€ ì •ìƒì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ì„ ê²½ìš° ëŒ€ë¹„
        if self.okt is None:
            raise RuntimeError("âŒ í˜•íƒœì†Œ ë¶„ì„ê¸°(Okt)ê°€ ì •ìƒì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        morphemes = self.okt.morphs(keyword)  # âœ… self.okt ì‚¬ìš© ê°€ëŠ¥

        analysis = {
            "is_valid": True,
            "morpheme_analysis": {},
            "needs_optimization": False
        }

        counts = {keyword: text.count(keyword)}
        for morpheme in morphemes:
            counts[morpheme] = text.count(morpheme)

        for word, count in counts.items():
            is_valid = 15 <= count <= 20
            if not is_valid:
                analysis["is_valid"] = False
                analysis["needs_optimization"] = True

            analysis["morpheme_analysis"][word] = {
                "count": count,
                "is_valid": is_valid,
                "status": "ì ì •" if is_valid else "ê³¼ë‹¤" if count > 20 else "ë¶€ì¡±"
            }

        return analysis

    def generate_content(self, prompt_data: dict) -> str:
        """LLMì„ ì‚¬ìš©í•˜ì—¬ ì¡°ê±´ì— ë§ëŠ” ì½˜í…ì¸  ìƒì„±"""
        try:
            prompt = self._generate_writing_prompt(prompt_data)
            response = self.llm.invoke(prompt)

            if isinstance(response, AIMessage):
                content = response.content
            elif isinstance(response, dict) and 'content' in response:
                content = response['content']
            elif isinstance(response, str):
                content = response
            else:
                content = str(response)

            # ğŸ”¹ ë””ë²„ê¹… ë©”ì‹œì§€ë¥¼ ë¡œê·¸ë¡œ ë³€ê²½ (ê¸°ë³¸ì ìœ¼ë¡œ ì¶œë ¥ë˜ì§€ ì•ŠìŒ)
            logger.debug(f"âœ… Perplexity API ì‘ë‹µ íƒ€ì…: {type(response)}")
            logger.debug(f"âœ… Perplexity API ì‘ë‹µ ë‚´ìš©: {content}")

            # ğŸ”¹ í˜•íƒœì†Œ ë¶„ì„ ìˆ˜í–‰
            analysis = self.analyze_morphemes(content, prompt_data["keyword"])

            # ğŸ”¹ ìµœì í™” í•„ìš” ì‹œ ìˆ˜í–‰
            if not analysis["is_valid"]:
                content = self.optimize_content(content, prompt_data["keyword"], analysis)

            # ğŸ”¹ ê¸€ììˆ˜ ë¶€ì¡± ì‹œ ê¸°ì¡´ ì†Œì œëª© í™•ì¥ ìˆ˜í–‰
            content = self.expand_existing_subtopics(content, prompt_data["keyword"], prompt_data["subtitles"])

            return content

        except Exception as e:
            logger.error(f"âŒ ì½˜í…ì¸  ìƒì„± ì˜¤ë¥˜: {str(e)}")
            return "âš ï¸ ì½˜í…ì¸  ìƒì„± ì‹¤íŒ¨"

    def search_news(self, keyword: str, subtopics: list) -> list:
        """í‚¤ì›Œë“œ + ì†Œì œëª© ê¸°ë°˜ìœ¼ë¡œ ë‰´ìŠ¤/í†µê³„ ìë£Œ ê²€ìƒ‰"""
        used_links = []
        search_terms = [keyword] + subtopics  # í‚¤ì›Œë“œ + ì¶”ì²œëœ ì†Œì œëª© ê¸°ë°˜ ê²€ìƒ‰

        for term in search_terms:
            try:
                conn = http.client.HTTPSConnection("google.serper.dev")
                payload = json.dumps({"q": term, "gl": "kr", "hl": "ko"})

                headers = {
                    'X-API-KEY': SERPER_API_KEY,  # ğŸ”’ secrets.tomlì—ì„œ ë¶ˆëŸ¬ì˜¨ API í‚¤ ì‚¬ìš©
                    'Content-Type': 'application/json'
                }

                conn.request("POST", "/news", payload, headers)
                res = conn.getresponse()
                data = res.read()

                response_json = json.loads(data.decode("utf-8"))
                news_results = response_json.get("news", [])

                if news_results:
                    # ğŸ”¹ ìµœëŒ€ 3ê°œ ê¸°ì‚¬ë§Œ ì‚¬ìš©
                    for article in news_results[:3]:
                        used_links.append(article.get("link", "URL ì—†ìŒ"))

            except Exception as e:
                print(f"âš ï¸ ë‰´ìŠ¤ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")

        return used_links  # ğŸ”¹ ìµœì¢…ì ìœ¼ë¡œ ìˆ˜ì§‘ëœ ì¶œì²˜ ë§í¬ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

        
    def _generate_writing_prompt(self, data: dict) -> str:
        """ìƒì„¸í•œ ì¡°ê±´ì„ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        keyword = data["keyword"]
        morphemes = self.okt.morphs(keyword)

        # ğŸ”¹ ì•ˆì „í•œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        difficulties = ', '.join(data.get("difficulties", ["ê´€ë ¨ ì–´ë ¤ì›€ ì—†ìŒ"]))
        business_name = data.get("business_name", "ìš°ë¦¬ íšŒì‚¬")
        expertise = data.get("expertise", "ì „ë¬¸ì ì¸ ì§€ì‹")

        prompt = f"""
        ë‹¤ìŒ ì¡°ê±´ì„ ì •í™•íˆ ì¤€ìˆ˜í•˜ì—¬ ë¸”ë¡œê·¸ ê¸€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:

        1. í•µì‹¬ í‚¤ì›Œë“œ ì‚¬ìš© ì¡°ê±´ (ê°€ì¥ ì¤‘ìš”)
        - ì£¼ í‚¤ì›Œë“œ: {keyword}
        - êµ¬ì„± í˜•íƒœì†Œ: {', '.join(morphemes)}
        - í•„ìˆ˜ ì¶œí˜„ íšŸìˆ˜: í‚¤ì›Œë“œì™€ ê° í˜•íƒœì†Œê°€ ê°ê° 15-20íšŒ ì¶œí˜„ (ctrl+f ê²€ìƒ‰ ê¸°ì¤€)
        - ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ë§¥ì—ì„œ ì‚¬ìš©í•  ê²ƒ
        
        2. ê¸€ì˜ êµ¬ì¡°
        - ì „ì²´ ê¸¸ì´: 2200-2500ì (ê³µë°± ì œì™¸)
        - êµ¬ì¡°: ì„œë¡ (20%) - ë³¸ë¡ (60%) - ê²°ë¡ (20%)

        3. ì½˜í…ì¸  ìš”êµ¬ì‚¬í•­
        - ì†Œì œëª©: {data.get('subtitles', [])}
        - íƒ€ê²Ÿ ë…ì: {data.get('target_audience', '')}
        - ì „ë¬¸ì„±: {expertise}

        4. í˜•íƒœì†Œ ì‚¬ìš© ì „ëµ
        - ì£¼ìš” ê°œë… ì„¤ëª… ì‹œ í‚¤ì›Œë“œ ì „ì²´ ì‚¬ìš©
        - ë°˜ë³µë˜ëŠ” ë§¥ë½ì—ì„œëŠ” ê°œë³„ í˜•íƒœì†Œ í™œìš©
        - ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ íë¦„ ìœ ì§€
        - ë¶ˆí•„ìš”í•œ ë°˜ë³µ ì—†ì´ ê³ ë¥´ê²Œ ë¶„í¬

        5. ì„œë¡ 
        - í˜ì¸í¬ì¸íŠ¸ ê³µê° í‘œí˜„
        - ì „ë¬¸ì„± ê°•ì¡°
        - ë¬¸ì œ ì¸ì‹ê³¼ í•´ê²°ì±… ì œì‹œ
        - ì˜ˆì‹œ) ì´ ê¸€ì„ ì½ëŠ” ì—¬ëŸ¬ë¶„ë“¤ì€ {keyword}ì˜ ì¤‘ìš”ì„±ì— ëŒ€í•´ ì˜ ì•Œê³  ê³„ì‹ ê°€ìš”? 
        {keyword}ëŠ” ~ì— ë§¤ìš° ì¤‘ìš”í•œ ì—­í• ì„ í•˜ì§€ë§Œ, ì‹¤ì œë¡œ {keyword}ë¥¼ ì œëŒ€ë¡œ ì´í•´í•˜ê³  ì‚¬ìš©í•˜ê±°ë‚˜ 
        ê´€ë¦¬í•˜ëŠ” ì‚¬ëŒë“¤ì€ ë§ì§€ ì•ŠìŠµë‹ˆë‹¤. 
        í˜¹ì‹œ ì§€ê¸ˆ {difficulties} ê°™ì€ ë¬¸ì œë¥¼ ê²ªê³  ê³„ì‹ ê°€ìš”?
        ê·¸ë ‡ë‹¤ë©´ {keyword}ê°€ ì›ì¸ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        í•˜ì§€ë§Œ ê±±ì • ë§ˆì‹œê³ , ì´ ê¸€ì— 5ë¶„ë§Œ ì§‘ì¤‘í•´ì£¼ì„¸ìš”. 
        ì €í¬ {business_name}ì´(ê°€) {expertise}ë¥¼ ë°”íƒ•ìœ¼ë¡œ 
        ë¯¿ì„ ìˆ˜ ìˆëŠ” ì •ë³´ë¥¼ ì œê³µí•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤. 
        ëê¹Œì§€ ì½ì–´ë³´ì‹œê³  '{difficulties}' ê°™ì€ ë¬¸ì œë¥¼ í•´ê²°í•´ë³´ì„¸ìš”.

        6. ë³¸ë¡ 
        - í†µê³„ ë°ì´í„°, ì‚¬ë¡€ ì—°êµ¬, ê¿€íŒ ë“± í™œìš©ìœ¼ë¡œ ì‹ ë¢°ë„ë¥¼ ë†’ì¼ ê²ƒ
        - í‚¤ì›Œë“œì™€ í˜•íƒœì†Œ í™œìš©
        - ê°€ë…ì„± ìˆëŠ” ë¬¸ì¥ êµ¬ì„±ê³¼ ê¸€ì˜ ìì—°ìŠ¤ëŸ¬ìš´ íë¦„ìœ¼ë¡œ ì„¤ë“ë ¥ì„ ë†’ì¼ ê²ƒ

        7. ê²°ë¡ 
        - ë³¸ë¡ ì˜ ì •ë³´ë“¤ ìš”ì•½ ë° ì •ë¦¬
        - í•´ê²°ì´ ì•ˆ ë  ê²½ìš° {business_name}ì— ë¬¸ì˜í•  ìˆ˜ ìˆê²Œë” ì•ˆë‚´
        - ê²€ìƒ‰ìì˜ {difficulties}ë¥¼ ì •ë§ í•´ê²°í•´ ì¤„ ìˆ˜ ìˆì„ì§€ì— ëŒ€í•œ ê²€ìƒ‰ìì˜ ë¶ˆì•ˆê°ì„ í•´ì†Œ 
        ìœ„ ì¡°ê±´ë“¤ì„ ëª¨ë‘ ì¶©ì¡±í•˜ëŠ” ì „ë¬¸ì ì´ê³  ìì—°ìŠ¤ëŸ¬ìš´ ê¸€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.
        íŠ¹íˆ í‚¤ì›Œë“œì™€ í˜•íƒœì†Œì˜ ì¶œí˜„ íšŸìˆ˜ë¥¼ ì •í™•íˆ ì§€ì¼œì£¼ì„¸ìš”.
        """
        
        return prompt

    from collections import Counter

    def optimize_content(self, text: str, keyword: str, analysis: dict) -> str:
        """LLMì„ ì‚¬ìš©í•˜ì—¬ ì½˜í…ì¸  ìµœì í™” (í˜•íƒœì†Œ & ê¸€ììˆ˜ ì¡°ì •)"""
        try:
            # ğŸ”¹ í˜•íƒœì†Œ ë¶„ì„ê¸° í™œìš©
            morphemes = self.okt.morphs(keyword)  # í‚¤ì›Œë“œë¥¼ êµ¬ì„±í•˜ëŠ” í˜•íƒœì†Œ ë¦¬ìŠ¤íŠ¸
            word_counts = Counter(self.okt.morphs(text))  # ì „ì²´ ê¸€ì˜ í˜•íƒœì†Œ ì¶œí˜„ íšŸìˆ˜
            
            optimization_needed = False
            adjustments = []

            # ğŸ”¹ í˜•íƒœì†Œë³„ ì¶œí˜„ íšŸìˆ˜ í™•ì¸ í›„ ìµœì í™” í•„ìš” ì—¬ë¶€ ê²°ì •
            for morpheme in morphemes:
                count = word_counts[morpheme]
                if count < 17 or count > 20:  # âœ… 17~20íšŒ ìœ ì§€ í•„ìš”
                    optimization_needed = True
                    adjustments.append(f"- `{morpheme}`: {count}íšŒ â†’ 17~20íšŒë¡œ ì¡°ì • í•„ìš”")

            # ğŸ”¹ í‚¤ì›Œë“œ ìì²´ ("ë¸Œë ˆì´í¬ ë¼ì´ë‹") ì¶œí˜„ íšŸìˆ˜ í™•ì¸
            keyword_count = text.count(keyword)
            if keyword_count < 17 or keyword_count > 20:
                optimization_needed = True
                adjustments.append(f"- `{keyword}`: {keyword_count}íšŒ â†’ 17~20íšŒë¡œ ì¡°ì • í•„ìš”")

            # ğŸ”¹ ê³µë°± ì œì™¸ ê¸€ììˆ˜ ê³„ì‚°
            text_length = len(text.replace(" ", ""))
            if text_length < 1500 or text_length > 2000:
                optimization_needed = True
                adjustments.append(f"- í˜„ì¬ ê¸€ììˆ˜: {text_length}ì â†’ 1500~2000ìë¡œ ì¡°ì • í•„ìš”")

            if not optimization_needed:
                return text  # âœ… ìµœì í™” í•„ìš” ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜

            # ğŸ”¹ ìµœì í™” í”„ë¡¬í”„íŠ¸ ìƒì„±
            optimization_prompt = f"""
            ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ìµœì í™”í•´ì£¼ì„¸ìš”.
            
            ğŸ”¹ í‚¤ì›Œë“œ: {keyword}
            ğŸ”¹ í˜•íƒœì†Œ êµ¬ì„±: {', '.join(morphemes)}
            ğŸ”¹ ì¡°ì • í•„ìš” í•­ëª©:
            {'\n'.join(adjustments)}
            
            âœ… ìš”êµ¬ì‚¬í•­:
            1. `{keyword}`ì™€ ê° í˜•íƒœì†Œ(`{', '.join(morphemes)}`)ê°€ 17~20íšŒ ë“±ì¥í•˜ë„ë¡ ì¡°ì •
            2. í‚¤ì›Œë“œ & í˜•íƒœì†ŒëŠ” ê¸€ì—ì„œ ê°€ì¥ ë§ì´ ì‚¬ìš©ëœ ë‹¨ì–´ì—¬ì•¼ í•¨
            3. ë™ì˜ì–´ë‚˜ ìœ ì‚¬ì–´ê°€ ìˆë‹¤ë©´ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€ì²´
            4. ë™ì˜ì–´ë‚˜ ìœ ì‚¬ì–´ê°€ ì—†ìœ¼ë©´ ë¬¸ì¥ì—ì„œ ì œê±° (ë¬¸ë§¥ìƒ ìì—°ìŠ¤ëŸ¬ìš¸ ê²½ìš°)
            5. ê³µë°± ì œì™¸ ê¸€ììˆ˜ë¥¼ 1500~2000ìë¡œ ë§ì¶”ê¸° (ë¶€ì¡±í•˜ë©´ ì¶”ê°€ ì„¤ëª…, ê¿€íŒ, ì‚¬ë¡€, FAQ í¬í•¨)
            6. ì „ì²´ ë¬¸ë§¥ì´ ìì—°ìŠ¤ëŸ½ë„ë¡ ìœ ì§€
            
            ì›ë¬¸:
            {text}
            """

            # ğŸ”¹ Perplexity API í˜¸ì¶œí•˜ì—¬ ìµœì í™” ìˆ˜í–‰
            response = self.llm.invoke(optimization_prompt)

            if isinstance(response, AIMessage):
                return response.content
            elif isinstance(response, dict) and 'content' in response:
                return response['content']
            elif isinstance(response, str):
                return response
            else:
                return str(response)

        except Exception as e:
            print(f"âŒ ì½˜í…ì¸  ìµœì í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return text

        
    def _format_analysis(self, analysis: dict) -> str:
        """ë¶„ì„ ê²°ê³¼ë¥¼ ì½ê¸° ì‰½ê²Œ í¬ë§·íŒ…"""
        result = "í˜•íƒœì†Œ ë¶„ì„ ê²°ê³¼:\n"
        for word, info in analysis["morpheme_analysis"].items():
            result += f"- {word}: {info['count']}íšŒ ({info['status']})\n"
        return result

    def _get_optimization_instructions(self, analysis: dict) -> str:
        """ìµœì í™” ì§€ì¹¨ ìƒì„±"""
        instructions = []
        for word, info in analysis["morpheme_analysis"].items():
            if not info["is_valid"]:
                if info["count"] > 20:
                    instructions.append(
                        f"- {word}: {info['count']}íšŒ â†’ 15-20íšŒë¡œ ê°ì†Œ í•„ìš”"
                    )
                else:
                    instructions.append(
                        f"- {word}: {info['count']}íšŒ â†’ 15-20íšŒë¡œ ì¦ê°€ í•„ìš”"
                    )
        return "\n".join(instructions)

    def count_chars(self, text: str) -> dict:
        """ê¸€ììˆ˜ ë¶„ì„"""
        text_without_spaces = text.replace(" ", "")
        count = len(text_without_spaces)
        return {
            "count": count,
            "is_valid": 2200 <= count <= 2500
        }

    def extract_statistics(self, text: str) -> List[Dict]:
        """í†µê³„ ë°ì´í„° ì¶”ì¶œ"""
        patterns = [
            r'(\d+(?:\.\d+)?%)',  # ë°±ë¶„ìœ¨
            r'(\d+(?:,\d{3})*(?:\.\d+)?)\s*(?:ëª…|ê°œ|ì›|ë‹¬ëŸ¬|ìœ„|ë°°|ì²œ|ë§Œ|ì–µ)',  # í•œê¸€ ë‹¨ìœ„
            r'(\d+(?:,\d{3})*(?:\.\d+)?)\s*(?:people|users|dollars|times|billion|million)'  # ì˜ë¬¸ ë‹¨ìœ„
        ]
        
        statistics = []
        for pattern in patterns:
            matches = re.finditer(pattern, text)
            for match in matches:
                start = max(0, match.start() - 50)
                end = min(len(text), match.end() + 50)
                context = text[start:end]
                
                statistics.append({
                    "statistic": match.group(),
                    "context": context.strip(),
                    "position": match.start()
                })
        
        return statistics
    
class BlogChainSystem:
    def __init__(self):
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        secrets = load_secrets()
        self.perplexity_api_key = secrets['api']['perplexity']
        self.analyzer = ContentAnalyzer()
        self.collected_references = []
        self.setup_chains()


    def setup_chains(self):
        secrets = load_secrets()
        self.perplexity_api_key = secrets['api']['perplexity']
        self.openai_api_key = secrets['api']['openai']
        
        self.llm = ChatPerplexity(
            model="sonar-pro",
            api_key=self.perplexity_api_key,
            temperature=0.7
        )
        
        # GPT-4 í’ˆì§ˆ ê²€ì‚¬ìš©
        self.quality_llm = ChatOpenAI(
            model="gpt-4o",
            api_key=self.openai_api_key,
            temperature=0
        )
        
        # SEO ë¶„ì„ ì²´ì¸
        seo_template = ChatPromptTemplate.from_template("""ë‹¹ì‹ ì€ SEO ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ í‚¤ì›Œë“œë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:
        í‚¤ì›Œë“œ: {keyword}

        ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë¶„ì„ ê²°ê³¼ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:

        1. ì£¼ìš” ê²€ìƒ‰ ì˜ë„: 
        (2-3ë¬¸ì¥ìœ¼ë¡œ ì´ í‚¤ì›Œë“œë¥¼ ê²€ìƒ‰í•˜ëŠ” ì‚¬ëŒë“¤ì˜ ì£¼ìš” ì˜ë„ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”)

        2. ê²€ìƒ‰ìê°€ ì–»ê³ ì í•˜ëŠ” ì •ë³´:
        (ê°€ì¥ ì¤‘ìš”í•œ 3ê°€ì§€ë§Œ bullet pointë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”)
        - 
        - 
        - 

        3. ê²€ìƒ‰ìê°€ ê²ªê³  ìˆëŠ” ë¶ˆí¸í•¨ì´ë‚˜ ì–´ë ¤ì›€:
        (ê°€ì¥ ì¼ë°˜ì ì¸ 3ê°€ì§€ ì–´ë ¤ì›€ë§Œ bullet pointë¡œ ê°„ë‹¨í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”)
        - 
        - 
        - 

        ëª¨ë“  ë‚´ìš©ì€ ê°„ë‹¨ëª…ë£Œí•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.""")
        self.seo_chain = seo_template | self.llm
        
        # ì‹œì¥ ì¡°ì‚¬ ì²´ì¸
        research_template = ChatPromptTemplate.from_template("""ë‹¹ì‹ ì€ ìŠ¤í¬ë©í•‘ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ í‚¤ì›Œë“œì— ëŒ€í•œ ìµœì‹  ì‹œì¥ ë°ì´í„°ì™€ êµ­ë‚´ì™¸ ê¸°ì‚¬ë¥¼ ì¡°ì‚¬í•´ì£¼ì„¸ìš”:
        í‚¤ì›Œë“œ: {keyword}

        ë‹¤ìŒ í•­ëª©ë“¤ì„ ì¡°ì‚¬í•´ì£¼ì„¸ìš”:
        1. ê²€ìƒ‰í•˜ëŠ” ì‚¬ëŒë“¤ì´ ì°¾ëŠ” ì •ë³´
        2. {keyword}ê´€ë ¨ëœ ì£¼ìš” ê¸°ì‚¬ë‚˜, í†µê³„ ë°ì´í„°
        3. ì†Œë¹„ì í–‰ë™ ë°ì´í„°
                                                             
        ì‘ë‹µ í˜•ì‹:
        ê° ë°ì´í„°ì— ëŒ€í•´ ë‹¤ìŒ ì •ë³´ë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”:
        - ë°ì´í„° ë‚´ìš©
        - ì¶œì²˜ URL
        """)
        self.research_chain = research_template | self.llm
        
        # í’ˆì§ˆ ê²€ì‚¬ ì²´ì¸
        self.quality_chain = ChatPromptTemplate.from_template(
            """ë‹¹ì‹ ì€ ë§¤ìš° ì—„ê²©í•œ ì½˜í…ì¸  í’ˆì§ˆ ê´€ë¦¬ìì…ë‹ˆë‹¤. 
            
            ë‹¤ìŒ ë¸”ë¡œê·¸ ë‚´ìš©ì„ ì—„ê²©í•˜ê²Œ ë¶„ì„í•˜ê³  ê° í•­ëª©ë³„ë¡œ ì •í™•í•œ ìˆ˜ì¹˜ì™€ í•¨ê»˜ ê²°ê³¼ë¥¼ ë³´ê³ í•´ì£¼ì„¸ìš”:
            
            ê²€ì‚¬ í•­ëª©:
            1. í˜•íƒœì†Œ ë¶„ì„ [í•„ìˆ˜]
            - í˜•íƒœì†Œ ì´ ì‚¬ìš© íšŸìˆ˜
            - 9~11íšŒ ì‚¬ìš© ê¸°ì¤€ ì¶©ì¡± ì—¬ë¶€
            - ë¯¸ì¶©ì¡± ì‹œ í˜„ì¬ íšŸìˆ˜ì™€ ëª©í‘œ íšŸìˆ˜ ì°¨ì´
            
            2. ê¸€ì ìˆ˜ ê²€ì¦ [í•„ìˆ˜]
            - ê³µë°± ì œì™¸ ì´ ê¸€ì ìˆ˜
            - 2200-2500ì ê¸°ì¤€ ì¶©ì¡± ì—¬ë¶€
            - ë¯¸ì¶©ì¡± ì‹œ í˜„ì¬ ê¸€ì ìˆ˜ì™€ ëª©í‘œ ë²”ìœ„ ì°¨ì´
            
            3. êµ¬ì¡° ë¶„ì„
            - ì„œë¡ (20%)/ë³¸ë¡ (60%)/ê²°ë¡ (20%) ë¹„ìœ¨ ê²€ì¦
            - ê° ì„¹ì…˜ë³„ ì‹¤ì œ ë¹„ìœ¨ ê³„ì‚°
            - 4ê°œì˜ ì†Œì œëª© í¬í•¨ ì—¬ë¶€
            
            4. ë°ì´í„° ê²€ì¦
            - ë³¸ë¡  ë‚´ ì •ëŸ‰ì  ë°ì´í„° ìµœì†Œ 2ê°œ ê²€ì¦
            - ëª¨ë“  í†µê³„ ë°ì´í„°ì˜ ì¶œì²˜ ëª…ì‹œ í™•ì¸
            - ì¶œì²˜ ëˆ„ë½ëœ ë°ì´í„° ëª©ë¡
            
            5. ì„œë¡  ìš”ì†Œ
            - í˜ì¸í¬ì¸íŠ¸ ê³µê° í‘œí˜„
            - ì „ë¬¸ì„± ê°•ì¡°
            - ë¬¸ì œ ì¸ì‹ê³¼ í•´ê²°ì±… ì œì‹œ

            6. ê²°ë¡  ìš”ì†Œ
            - ì„œë¹„ìŠ¤ ìì—°ìŠ¤ëŸ¬ìš´ ì†Œê°œ
            - ì „ë¬¸ì„± ê¸°ë°˜ ì‹ ë¢°ê°
            - êµ¬ì²´ì  ë‹¤ìŒ ë‹¨ê³„ ì œì‹œ
            
            ë¶„ì„í•  ë¸”ë¡œê·¸ ë‚´ìš©:
            {prompt}
            
            ê° í•­ëª©ì„ Pass/Failë¡œ íŒì •í•˜ê³ , Fail í•­ëª©ì— ëŒ€í•´ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ê°œì„  ì§€ì¹¨ì„ ì œì‹œí•˜ì„¸ìš”:
            1. [ë¬¸ì œ í•­ëª©]
            2. [í˜„ì¬ ìƒíƒœ]
            3. [ëª©í‘œ ìƒíƒœ]
            4. [ê°œì„  í–‰ë™ ì§€ì¹¨]
            
            ì „ì²´ í•­ëª© ì¤‘ í•˜ë‚˜ë¼ë„ Failì´ë©´ ë¶€ì í•© íŒì •ì…ë‹ˆë‹¤."""
        ) | self.quality_llm

    def handle_conversation(self):
        """ëŒ€í™” ìƒíƒœì— ë”°ë¥¸ í•¸ë“¤ëŸ¬ ì‹¤í–‰"""
        state = st.session_state.conversation_state
        
        if state.step == "keyword":
            self.handle_keyword_step()
        elif state.step == "seo_analysis":
            self.handle_seo_analysis_step()
        elif state.step == "subtopics":
            self.handle_subtopics_step()
        elif state.step == "business_info":
            self.handle_business_info_step()
        elif state.step == "target_audience":
            self.handle_target_audience_step()
        elif state.step == "morphemes":
            self.handle_morphemes_step()
        elif state.step == "reference":
            self.handle_reference_step()
        elif state.step == "content_creation":
            self.handle_content_creation_step()

    def search_related_articles(self, keyword: str, subtopics: List[str]) -> List[dict]:
        """í‚¤ì›Œë“œì™€ ì†Œì œëª©ì„ ê¸°ë°˜ìœ¼ë¡œ ê´€ë ¨ ê¸°ì‚¬ ë° í†µê³„ë¥¼ ê²€ìƒ‰"""
        try:
            search_queries = [keyword] + subtopics  # í‚¤ì›Œë“œ + ì†Œì œëª©ìœ¼ë¡œ ê²€ìƒ‰
            articles = []

            for query in search_queries:
                # ğŸ”¹ Perplexity ê²€ìƒ‰
                perplexity_prompt = f"'{query}'ì— ëŒ€í•œ ìµœì‹  ê¸°ì‚¬, ë…¼ë¬¸ ë˜ëŠ” í†µê³„ë¥¼ ì°¾ì•„ì„œ ìš”ì•½í•´ì£¼ì„¸ìš”."
                response = self.llm.invoke(perplexity_prompt)
                if isinstance(response, AIMessage):
                    articles.append({"source": "Perplexity", "content": response.content})
                elif isinstance(response, dict) and 'content' in response:
                    articles.append({"source": "Perplexity", "content": response['content']})

                # ğŸ”¹ Serper ê²€ìƒ‰
                conn = http.client.HTTPSConnection("google.serper.dev")
                payload = json.dumps({"q": query, "gl": "kr", "hl": "ko"})
                headers = {'X-API-KEY': SERPER_API_KEY, 'Content-Type': 'application/json'}
                conn.request("POST", "/news", payload, headers)
                res = conn.getresponse()
                data = res.read()

                try:
                    response_json = json.loads(data.decode("utf-8"))
                    news_results = response_json.get("news", [])
                    if news_results:
                        top_article = news_results[0]  # ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ê¸°ì‚¬ 1ê°œ ì„ íƒ
                        articles.append({
                            "source": "Serper",
                            "title": top_article.get("title", "ì œëª© ì—†ìŒ"),
                            "link": top_article.get("link", "URL ì—†ìŒ"),
                            "summary": top_article.get("snippet", "ìš”ì•½ ì—†ìŒ")
                        })
                except Exception as e:
                    print(f"âš ï¸ Serper ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

            return articles

        except Exception as e:
            print(f"âŒ ê¸°ì‚¬ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return []

    def handle_content_creation_step(self):
        st.markdown("### ë¸”ë¡œê·¸ ì‘ì„±")
        state = st.session_state.conversation_state
        
        try:
            # í•„ìš”í•œ ë°ì´í„° ìˆ˜ì§‘
            data = {
                "keyword": state.data.get('keyword', ''),
                "subtitles": state.data.get('subtopics', 
                            state.data.get('recommended_subtopics', [])),
                "target_audience": state.data.get('target_audience', {}).get('primary', ''),
                "expertise": state.data.get('business_info', {}).get('expertise', ''),
                "pain_points": state.data.get('target_audience', {}).get('pain_points', []),
                "business_name": state.data.get('business_info', {}).get('name', '')
            }

            if not data["keyword"]:
                st.error("í‚¤ì›Œë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return

            progress_messages = [
                "âœ¨ ë¸”ë¡œê·¸ êµ¬ì¡°ë¥¼ ì„¤ê³„í•˜ê³  ìˆìŠµë‹ˆë‹¤...",
                "ğŸ“Š ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...",
                "âœï¸ ì „ë¬¸ì„± ìˆëŠ” ì½˜í…ì¸ ë¥¼ ì‘ì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."
            ]

            with st.spinner("ë¸”ë¡œê·¸ ì‘ì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤..."):
                for msg in progress_messages:
                    st.write(msg)
                
                # ì»¨í…ì¸  ìƒì„±
                content_result = self.analyzer.generate_content(data)
                
                # ë¶„ì„ ìˆ˜í–‰
                morpheme_analysis = self.analyzer.analyze_morphemes(content_result, data["keyword"])
                chars_analysis = self.analyzer.count_chars(content_result)
                
                # ìµœì í™” í•„ìš” ì—¬ë¶€ í™•ì¸
                if (not morpheme_analysis["is_valid"] or not chars_analysis["is_valid"]):
                    st.info("ìƒì„±ëœ ì½˜í…ì¸ ë¥¼ ìµœì í™”í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
                    content_result = self.analyzer.optimize_content(
                        content_result, 
                        data["keyword"], 
                        morpheme_analysis
                    )
                
                # í’ˆì§ˆ ê²€ì‚¬
                check_result = self._quality_check(content_result)

                # ê²°ê³¼ í‘œì‹œ
                st.markdown("### ìµœì¢… ë¸”ë¡œê·¸ ë‚´ìš©")
                st.write(content_result)
                
                with st.expander("ë¶„ì„ ê²°ê³¼ ë³´ê¸°"):
                    st.write("í˜•íƒœì†Œ ë¶„ì„:")
                    for word, info in morpheme_analysis["morpheme_analysis"].items():
                        st.write(f"- {word}: {info['count']}íšŒ ({info['status']})")
                    
                    st.write(f"\nê¸€ììˆ˜: {chars_analysis['count']}ì")
                    st.write("\ní’ˆì§ˆ ê²€ì‚¬ ê²°ê³¼:")
                    st.write(check_result)
                
                # ì˜µì…˜ ë²„íŠ¼
                col1, col2 = st.columns(2)
                with col1:
                    if st.button("ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì‹œë„", key="restart_button"):
                        st.session_state.conversation_state = ConversationState()
                        st.rerun()
                with col2:
                    if st.button("ì´ëŒ€ë¡œ ì‚¬ìš©í•˜ê¸°", key="use_as_is_button"):
                        st.success("ë¸”ë¡œê·¸ ê¸€ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")

        except Exception as e:
            st.error(f"ì½˜í…ì¸  ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            print(f"ğŸ”¥ ë””ë²„ê¹…: {str(e)}")

    def insert_news_sources(self, content: str, scraped_links: list) -> str:
        """ì‚¬ìš©ëœ ë‰´ìŠ¤ ì¶œì²˜ë¥¼ ì½˜í…ì¸  ìµœí•˜ë‹¨ì— ì¶”ê°€"""

        if not scraped_links:
            return content  # ğŸ”¹ ì‚¬ìš©ëœ ë§í¬ê°€ ì—†ìœ¼ë©´ ì›ë³¸ ê·¸ëŒ€ë¡œ ë°˜í™˜

        # ğŸ”¹ ê¸€ì—ì„œ ì‹¤ì œë¡œ ì–¸ê¸‰ëœ ì¶œì²˜ë§Œ í¬í•¨
        used_links = [link for link in scraped_links if link in content]

        if not used_links:
            return content  # ğŸ”¹ ì¶œì²˜ê°€ í¬í•¨ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ì¶”ê°€í•˜ì§€ ì•ŠìŒ

        # ğŸ”¹ ê°€ë…ì„± ì¢‹ì€ ì¶œì²˜ í˜•ì‹ ìƒì„±
        sources_text = "\n\nğŸ”— **ì¶œì²˜:**\n"
        for idx, link in enumerate(used_links, start=1):
            sources_text += f"{idx}. {link}\n"

        return content + sources_text  # ğŸ”¹ ì›ë³¸ ì½˜í…ì¸ ì— ì¶œì²˜ ì •ë³´ ì¶”ê°€


    def _quality_check(self, content: str) -> str:
        """í’ˆì§ˆ ê²€ì‚¬ ìˆ˜í–‰"""
        try:
            response = self.quality_chain.invoke({"prompt": content})
            return str(response.content) if hasattr(response, 'content') else str(response)
        except Exception as e:
            return f"í’ˆì§ˆ ê²€ì‚¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        
    def handle_keyword_step(self):
        """í‚¤ì›Œë“œ ì…ë ¥ ë° ë¶„ì„ ë‹¨ê³„"""
        st.markdown("### ë¸”ë¡œê·¸ ì£¼ì œ ì„ ì •")
        keyword = st.text_input("ì–´ë–¤ ì£¼ì œë¡œ ë¸”ë¡œê·¸ë¥¼ ì‘ì„±í•˜ê³  ì‹¶ìœ¼ì‹ ê°€ìš”?")
        
        if keyword:
            with st.spinner("í‚¤ì›Œë“œ ë¶„ì„ ì¤‘..."):
                try:
                    analysis_result = self.seo_chain.invoke({"keyword": keyword})
                    result_text = str(analysis_result.content) if hasattr(analysis_result, 'content') else str(analysis_result)
                    
                    # ë¶„ì„ ê²°ê³¼ì—ì„œ ì‹¤ì œ ë‚´ìš©ë§Œ ì¶”ì¶œ
                    if "content='" in result_text:
                        result_text = result_text.split("content='")[1].split("additional_kwargs")[0]
                    
                    # ì´ìŠ¤ì¼€ì´í”„ëœ ì¤„ë°”ê¿ˆ ì²˜ë¦¬
                    result_text = result_text.replace('\\n', '\n').strip()
                    
                    # ì£¼ìš” ì •ë³´ ì¶”ì¶œ
                    main_intent = ""
                    pain_points = []
                    
                    # ì£¼ìš” ê²€ìƒ‰ ì˜ë„ ì¶”ì¶œ
                    if "1. ì£¼ìš” ê²€ìƒ‰ ì˜ë„:" in result_text:
                        intent_section = result_text.split("1. ì£¼ìš” ê²€ìƒ‰ ì˜ë„:")[1].split("2.")[0]
                        main_intent = intent_section.strip()
                    
                    # ì–´ë ¤ì›€/ë¶ˆí¸í•¨ ì¶”ì¶œ
                    if "3. ê²€ìƒ‰ìê°€ ê²ªê³  ìˆëŠ” ë¶ˆí¸í•¨ì´ë‚˜ ì–´ë ¤ì›€:" in result_text:
                        difficulties = result_text.split("3. ê²€ìƒ‰ìê°€ ê²ªê³  ìˆëŠ” ë¶ˆí¸í•¨ì´ë‚˜ ì–´ë ¤ì›€:")[1].split("\n")
                        pain_points = [d.strip().replace('- ', '') for d in difficulties if d.strip().startswith('-')]
                    
                    # ë¶„ì„ ë°ì´í„° ì €ì¥
                    analysis_data = {
                        'raw_text': result_text,
                        'main_intent': main_intent,
                        'pain_points': pain_points
                    }
                    
                    st.session_state.conversation_state.data['keyword'] = keyword
                    st.session_state.conversation_state.data['keyword_analysis'] = analysis_data
                    st.session_state.conversation_state.step = "seo_analysis"
                    st.rerun()
                    
                except Exception as e:
                    st.error(f"í‚¤ì›Œë“œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

    def handle_seo_analysis_step(self):
        st.markdown("### ê²€ìƒ‰ íŠ¸ë Œë“œ ë¶„ì„ ê²°ê³¼")
        
        # ë¶„ì„ ê²°ê³¼ ì²˜ë¦¬
        raw_analysis = st.session_state.conversation_state.data.get('keyword_analysis', '')
        
        try:
            if raw_analysis:
                # content ë¶€ë¶„ ì¶”ì¶œ ë° í¬ë§·íŒ…
                analysis_text = str(raw_analysis.get('raw_text', ''))
                
                # í…ìŠ¤íŠ¸ í¬ë§·íŒ…
                formatted_text = ""
                for line in analysis_text.split('\n'):
                    if line.startswith('1. ì£¼ìš” ê²€ìƒ‰ ì˜ë„:'):
                        formatted_text += f"{line}\n\n"
                    elif line.startswith('3. ê²€ìƒ‰ìê°€ ê²ªê³  ìˆëŠ” ë¶ˆí¸í•¨ì´ë‚˜ ì–´ë ¤ì›€:'):
                        formatted_text += f"{line}\n"
                        difficulties = raw_analysis.get('pain_points', [])
                        for diff in difficulties:
                            formatted_text += f"- {diff}\n"
                    elif line.strip() and line.startswith('-'):
                        formatted_text += f"{line}\n"
                    elif line.strip():
                        formatted_text += f"{line}\n\n"
                
                # ë¶„ì„ ê²°ê³¼ í‘œì‹œ
                st.write(formatted_text)
                
                # ì†Œì œëª© ìƒì„±
                keyword = st.session_state.conversation_state.data['keyword']
                try:
                    # ì‹œì¥ ì¡°ì‚¬ ì‹¤í–‰
                    research_result = self.research_chain.invoke({"keyword": keyword})
                    st.session_state.conversation_state.data['market_research'] = str(research_result)
                    
                    # ì†Œì œëª© ì¶”ì²œ
                    subtopics_prompt = f"""
                    ê²€ìƒ‰ í‚¤ì›Œë“œ '{keyword}'ì— ëŒ€í•œ ë¸”ë¡œê·¸ ì†Œì œëª© 4ê°œë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”.

                    ì¡°ê±´:
                    1. ëª¨ë“  ì†Œì œëª©ì€ ë°˜ë“œì‹œ '{keyword}'ì™€ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ë˜ì–´ì•¼ í•¨
                    2. ì†Œì œëª©ë“¤ì€ ë…¼ë¦¬ì  ìˆœì„œë¡œ êµ¬ì„±
                    3. ê° ì†Œì œëª©ì€ ê²€ìƒ‰ìì˜ ì‹¤ì œ ê³ ë¯¼/ê¶ê¸ˆì¦ì„ í•´ê²°í•  ìˆ˜ ìˆëŠ” ë‚´ìš©
                    4. ì „ì²´ì ìœ¼ë¡œ '{keyword}'ì— ëŒ€í•œ í¬ê´„ì  ì´í•´ë¥¼ ì œê³µí•  ìˆ˜ ìˆëŠ” êµ¬ì„±

                    í˜•ì‹:
                    1. [ì²« ë²ˆì§¸ ì†Œì œëª©]: ê¸°ì´ˆ/ê°œìš”
                    2. [ë‘ ë²ˆì§¸ ì†Œì œëª©]: ì£¼ìš” ì •ë³´/íŠ¹ì§•
                    3. [ì„¸ ë²ˆì§¸ ì†Œì œëª©]: ì‹¤ìš©ì  íŒ/ë°©ë²•
                    4. [ë„¤ ë²ˆì§¸ ì†Œì œëª©]: ì„ íƒ/ê´€ë¦¬ ë°©ë²•
                    """
                    
                    subtopics_result = self.llm.invoke(subtopics_prompt)
                    subtopics_content = str(subtopics_result.content) if hasattr(subtopics_result, 'content') else str(subtopics_result)
                    
                    st.session_state.conversation_state.data['recommended_subtopics'] = subtopics_content
                    
                    st.markdown("### âœï¸ ì¶”ì²œ ì†Œì œëª©")
                    st.write(subtopics_content)
                    
                    col1, col2 = st.columns(2)
                    with col1:
                        if st.button("ì´ ì†Œì œëª©ë“¤ë¡œ ì§„í–‰í•˜ê¸°", key="accept_subtopics"):
                            st.session_state.conversation_state.step = "business_info"
                            st.rerun()
                    with col2:
                        if st.button("ì†Œì œëª© ì§ì ‘ ì…ë ¥í•˜ê¸°", key="custom_subtopics"):
                            st.session_state.conversation_state.step = "subtopics"
                            st.rerun()
                            
                except Exception as e:
                    st.error(f"ì†Œì œëª© ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
                    
        except Exception as e:
            st.error(f"ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

    def handle_subtopics_step(self):
        """ì†Œì œëª© ì§ì ‘ ì…ë ¥ ë‹¨ê³„"""
        st.markdown("### ì†Œì œëª© ìˆ˜ì •")
        st.write("ì¶”ì²œëœ ì†Œì œëª©ì„ ìˆ˜ì •í•˜ê±°ë‚˜ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        # ì´ì „ ë‹¨ê³„ì˜ ì¶”ì²œ ì†Œì œëª© ê°€ì ¸ì˜¤ê¸°
        keyword_analysis = st.session_state.conversation_state.data.get('recommended_subtopics', '')
        
        # ì¶”ì²œëœ ì†Œì œëª©ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        recommended_subtopics = []
        if keyword_analysis:
            lines = keyword_analysis.split('\n')
            for line in lines:
                line = line.strip()
                if line and line[0].isdigit() and '. ' in line:
                    # ë²ˆí˜¸ë§Œ ì œê±°í•˜ê³  ë‚˜ë¨¸ì§€ í˜•ì‹ì€ ìœ ì§€
                    subtitle = line.split('. ', 1)[1]
                    if subtitle:
                        recommended_subtopics.append(subtitle)
        
        # 4ê°œì˜ ì†Œì œëª©ì´ ë˜ë„ë¡ ë¹ˆ ë¬¸ìì—´ë¡œ ì±„ì›€
        while len(recommended_subtopics) < 4:
            recommended_subtopics.append('')
            
        # 4ê°œì˜ ì†Œì œëª© ì…ë ¥ í•„ë“œ ìƒì„±
        subtopics = []
        for i in range(4):
            default_value = recommended_subtopics[i] if i < len(recommended_subtopics) else ''
            subtopic = st.text_input(
                f"ì†Œì œëª© {i+1}",
                value=default_value,
                help="ì›í•˜ì‹œëŠ” ëŒ€ë¡œ ìˆ˜ì •í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            )
            if subtopic:
                subtopics.append(subtopic)
        
        col1, col2 = st.columns(2)
        with col1:
            if len(subtopics) == 4 and st.button("ì†Œì œëª© í™•ì •"):
                st.session_state.conversation_state.data['subtopics'] = subtopics
                st.session_state.conversation_state.step = "business_info"
                st.rerun()
        with col2:
            if st.button("ì´ì „ ë‹¨ê³„ë¡œ"):
                st.session_state.conversation_state.step = "seo_analysis"
                st.rerun()

    def handle_business_info_step(self):
        """ì‚¬ì—…ì ì •ë³´ ì…ë ¥ ë‹¨ê³„"""
        st.markdown("### ì‚¬ì—…ì ì •ë³´ ì…ë ¥")
        st.write("ë¸”ë¡œê·¸ì— í¬í•¨ë  ì‚¬ì—…ì ì •ë³´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        
        business_name = st.text_input("ìƒí˜¸ëª…ì„ ì…ë ¥í•´ì£¼ì„¸ìš”", 
                                    help="ì˜ˆì‹œ: ë””ì§€í„¸ë§ˆì¼€íŒ…ì—°êµ¬ì†Œ")
        expertise = st.text_input("ì „ë¬¸ì„±ì„ ì…ë ¥í•´ì£¼ì„¸ìš”", 
                                help="ì˜ˆì‹œ: 10ë…„ ê²½ë ¥ì˜ ë§ˆì¼€íŒ… ì „ë¬¸ê°€, 100ê°œ ì´ìƒì˜ í”„ë¡œì íŠ¸ ìˆ˜í–‰")
        
        if business_name and expertise and st.button("ë‹¤ìŒ ë‹¨ê³„ë¡œ"):
            st.session_state.conversation_state.data['business_info'] = {
                "name": business_name,
                "expertise": expertise
            }
            st.session_state.conversation_state.step = "target_audience"
            st.rerun()

    def handle_target_audience_step(self):
        """íƒ€ê²Ÿ ë…ìì¸µ ì„¤ì • ë‹¨ê³„"""
        st.markdown("### íƒ€ê²Ÿ ë…ìì¸µ ì„¤ì •")
        
        # í‚¤ì›Œë“œ ë¶„ì„ ê²°ê³¼ì—ì„œ ì •ë³´ ì¶”ì¶œ
        analysis_data = st.session_state.conversation_state.data.get('keyword_analysis', {})
        raw_text = analysis_data.get('raw_text', '')
        default_target = ""
        default_pain_points = ""
        
        if raw_text:
            try:
                # ì£¼ìš” ê²€ìƒ‰ ì˜ë„ ì¶”ì¶œ
                if "ì£¼ìš” ê²€ìƒ‰ ì˜ë„:" in raw_text:
                    sections = raw_text.split("2. ")[0]
                    if "1. ì£¼ìš” ê²€ìƒ‰ ì˜ë„:" in sections:
                        default_target = sections.split("1. ì£¼ìš” ê²€ìƒ‰ ì˜ë„:")[1].strip()
                
                # ì–´ë ¤ì›€/ë¶ˆí¸í•¨ ì¶”ì¶œ
                if "3. ê²€ìƒ‰ìê°€ ê²ªê³  ìˆëŠ” ë¶ˆí¸í•¨ì´ë‚˜ ì–´ë ¤ì›€:" in raw_text:
                    difficulties_section = raw_text.split("3. ê²€ìƒ‰ìê°€ ê²ªê³  ìˆëŠ” ë¶ˆí¸í•¨ì´ë‚˜ ì–´ë ¤ì›€:")[1]
                    difficulties = []
                    for line in difficulties_section.split('\n'):
                        if line.strip().startswith('- '):
                            difficulties.append(line.replace('- ', ''))
                    default_pain_points = '\n'.join(difficulties)
            
            except Exception as e:
                st.error(f"ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        
        primary_target = st.text_input(
            "ì£¼ìš” íƒ€ê²Ÿ ë…ìì¸µì„ ì…ë ¥í•´ì£¼ì„¸ìš”",
            value=default_target,
            help="ì˜ˆì‹œ: ì†Œìƒê³µì¸, ìŠ¤íƒ€íŠ¸ì—… ëŒ€í‘œ, ë§ˆì¼€íŒ… ë‹´ë‹¹ì"
        )
        
        pain_points = st.text_area(
            "íƒ€ê²Ÿ ë…ìì¸µì´ ê²ªëŠ” ì–´ë ¤ì›€ì„ ì…ë ¥í•´ì£¼ì„¸ìš”",
            value=default_pain_points,
            help="ì˜ˆì‹œ: ë§ˆì¼€íŒ… ë¹„ìš© ë¶€ë‹´, ê³ ê° í™•ë³´ì˜ ì–´ë ¤ì›€"
        )
        
        st.write("ì¶”ê°€ íƒ€ê²Ÿ ë…ìì¸µì´ ìˆë‹¤ë©´ ì…ë ¥í•´ì£¼ì„¸ìš” (ì„ íƒì‚¬í•­)")
        additional_target = st.text_input("ì¶”ê°€ íƒ€ê²Ÿ ë…ìì¸µ")
        
        if primary_target and pain_points and st.button("ë‹¤ìŒ ë‹¨ê³„ë¡œ"):
            target_info = {
                "primary": primary_target,
                "pain_points": pain_points.split('\n'),
                "additional": additional_target if additional_target else None
            }
            st.session_state.conversation_state.data['target_audience'] = target_info
            st.session_state.conversation_state.step = "morphemes"
            st.rerun()

    def handle_morphemes_step(self):
        """í•µì‹¬ í˜•íƒœì†Œ ì„¤ì • ë‹¨ê³„"""
        st.markdown("### í•µì‹¬ í˜•íƒœì†Œ ì„¤ì •")
        st.write("ë¸”ë¡œê·¸ì— ê¼­ í¬í•¨ë˜ì—ˆìœ¼ë©´ í•˜ëŠ” í•µì‹¬ ë‹¨ì–´ë‚˜ í˜•íƒœì†Œê°€ ìˆë‚˜ìš”? (ì„ íƒì‚¬í•­)")
        st.write("ì‰¼í‘œ(,)ë¡œ êµ¬ë¶„í•˜ì—¬ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        
        morphemes_input = st.text_input("í•µì‹¬ í˜•íƒœì†Œ", help="ì˜ˆì‹œ: ë§ˆì¼€íŒ…,ì„±ê³¼,ì „ëµ,ì†”ë£¨ì…˜")
        
        if st.button("ë‹¤ìŒ ë‹¨ê³„ë¡œ"):
            if morphemes_input:
                morphemes = [m.strip() for m in morphemes_input.split(",")]
                st.session_state.conversation_state.data['morphemes'] = morphemes
            st.session_state.conversation_state.step = "reference"
            st.rerun()

    def handle_reference_step(self):
        """ì°¸ê³  ìë£Œ ë¶„ì„ ë‹¨ê³„"""
        st.markdown("### ì°¸ê³  ìë£Œ ë¶„ì„")
        st.write("ì°¸ê³ í•˜ê³  ì‹¶ì€ ë¸”ë¡œê·¸ë‚˜ ê¸°ì‚¬ì˜ URLì´ ìˆë‹¤ë©´ ì…ë ¥í•´ì£¼ì„¸ìš”. (ì„ íƒì‚¬í•­)")
        
        reference_url = st.text_input("ì°¸ê³  URL (ì„ íƒì‚¬í•­)")
        
        if reference_url:
            with st.spinner("ì°¸ê³  ìë£Œ ë¶„ì„ ì¤‘..."):
                try:
                    reference_prompt = f"""
                    ë‹¤ìŒ URLì˜ ì½˜í…ì¸ ë¥¼ ë¶„ì„í•˜ì„¸ìš”:
                    URL: {reference_url}
                    
                    ë‹¤ìŒ í•­ëª©ë“¤ì„ ë¶„ì„í•´ì£¼ì„¸ìš”:
                    1. ë„ì…ë¶€(í›…) ë°©ì‹
                    2. ì½˜í…ì¸  êµ¬ì¡°
                    3. ìŠ¤í† ë¦¬í…”ë§ ë°©ì‹
                    4. ê²°ë¡  ì „ê°œ ë°©ì‹
                    5. ì£¼ìš” ì„¤ë“ í¬ì¸íŠ¸
                    6. ì •ëŸ‰ì  ë°ì´í„° ë° ì¶œì²˜
                    """
                    
                    analysis_result = self.llm.predict(reference_prompt)
                    st.session_state.conversation_state.data['reference_analysis'] = str(analysis_result)
                    st.session_state.conversation_state.data['reference_url'] = reference_url
                except Exception as e:
                    st.error(f"ì°¸ê³  ìë£Œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        
        col1, col2 = st.columns(2)
        with col1:
            if st.button("ğŸ‘‰ ì°¸ê³ ìë£Œ ì—†ì´ ë°”ë¡œ ì‘ì„±"):
                st.session_state.conversation_state.step = "content_creation"
                st.rerun()
        with col2:
            if reference_url and st.button("ğŸ‘‰ ì°¸ê³ ìë£Œì™€ í•¨ê»˜ ì‘ì„±"):
                st.session_state.conversation_state.step = "content_creation"
                st.rerun()

    def reset_conversation(self):
        """ëŒ€í™” ìƒíƒœë¥¼ ì´ˆê¸°í™”"""
        st.session_state.conversation_state = ConversationState()

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    st.set_page_config(page_title="ğŸ§AI ë¸”ë¡œê·¸ ì¹˜íŠ¸í‚¤", layout="wide")
    
    # conversation_state ì´ˆê¸°í™”
    if 'conversation_state' not in st.session_state:
        st.session_state.conversation_state = ConversationState()
    
    # system ì´ˆê¸°í™”
    if 'system' not in st.session_state:
        st.session_state.system = BlogChainSystem()

    st.title("ğŸ§AI ë¸”ë¡œê·¸ ì¹˜íŠ¸í‚¤")
    
    # ì‚¬ì´ë“œë°”ì— ì§„í–‰ ìƒí™© í‘œì‹œ
    with st.sidebar:
        st.markdown("### ì§„í–‰ ìƒí™©")
        steps = {
            "keyword": "1. í‚¤ì›Œë“œ ì„ ì •",
            "seo_analysis": "2. ê²€ìƒ‰ íŠ¸ë Œë“œ ë¶„ì„",
            "subtopics": "3. ì†Œì œëª© ì„ ì •",
            "business_info": "4. ì‚¬ì—…ì ì •ë³´",
            "target_audience": "5. íƒ€ê²Ÿ ë…ì",
            "morphemes": "6. í•µì‹¬ í˜•íƒœì†Œ",
            "reference": "7. ì°¸ê³  ìë£Œ",
            "content_creation": "8. ë¸”ë¡œê·¸ ì‘ì„±"
        }
        
        current_step = st.session_state.conversation_state.step
        for step, label in steps.items():
            if step == current_step:
                st.markdown(f"**â†’ {label}**")
            elif list(steps.keys()).index(step) < list(steps.keys()).index(current_step):
                st.markdown(f"âœ“ {label}")
            else:
                st.markdown(f"  {label}")
        
        if st.button("ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì‹œì‘"):
            st.session_state.system.reset_conversation()
            st.rerun()
    
    # ë©”ì¸ ì»¨í…ì¸ 
    st.session_state.system.handle_conversation()

if __name__ == "__main__":
    main()